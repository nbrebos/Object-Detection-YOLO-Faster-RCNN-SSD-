{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101038,"status":"ok","timestamp":1731697350577,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"},"user_tz":-120},"id":"kauOILHSqXWw","outputId":"da9d09b2-8f3a-4055-c8b4-56dcb0181bc7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting fiftyone\n","  Downloading fiftyone-1.0.2-py3-none-any.whl.metadata (12 kB)\n","Collecting aiofiles (from fiftyone)\n","  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n","Collecting argcomplete (from fiftyone)\n","  Downloading argcomplete-3.5.1-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.12.3)\n","Collecting boto3 (from fiftyone)\n","  Downloading boto3-1.35.62-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.5.0)\n","Collecting dacite<1.8.0,>=1.6.0 (from fiftyone)\n","  Downloading dacite-1.7.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: Deprecated in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.2.14)\n","Collecting ftfy (from fiftyone)\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.11.0)\n","Collecting hypercorn>=0.13.2 (from fiftyone)\n","  Downloading hypercorn-0.17.3-py3-none-any.whl.metadata (5.4 kB)\n","Requirement already satisfied: Jinja2>=3 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.1.4)\n","Collecting kaleido!=0.2.1.post1 (from fiftyone)\n","  Downloading kaleido-0.4.1-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fiftyone) (3.8.0)\n","Collecting mongoengine==0.24.2 (from fiftyone)\n","  Downloading mongoengine-0.24.2-py3-none-any.whl.metadata (6.7 kB)\n","Collecting motor>=2.5 (from fiftyone)\n","  Downloading motor-3.6.0-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fiftyone) (24.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2.2.2)\n","Requirement already satisfied: Pillow>=6.2 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (11.0.0)\n","Requirement already satisfied: plotly>=4.14 in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.24.1)\n","Collecting pprintpp (from fiftyone)\n","  Downloading pprintpp-0.4.0-py2.py3-none-any.whl.metadata (7.9 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from fiftyone) (5.9.5)\n","Collecting pymongo<4.9,>=3.12 (from fiftyone)\n","  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2024.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from fiftyone) (6.0.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fiftyone) (2024.9.11)\n","Collecting retrying (from fiftyone)\n","  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.5.2)\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.24.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fiftyone) (1.13.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from fiftyone) (75.1.0)\n","Collecting sseclient-py<2,>=1.7.2 (from fiftyone)\n","  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n","Collecting sse-starlette<1,>=0.10.3 (from fiftyone)\n","  Downloading sse_starlette-0.10.3-py3-none-any.whl.metadata (4.3 kB)\n","Collecting starlette>=0.24.0 (from fiftyone)\n","  Downloading starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n","Collecting strawberry-graphql (from fiftyone)\n","  Downloading strawberry_graphql-0.248.1-py3-none-any.whl.metadata (7.8 kB)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fiftyone) (0.9.0)\n","Collecting xmltodict (from fiftyone)\n","  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n","Collecting universal-analytics-python3<2,>=1.0.1 (from fiftyone)\n","  Downloading universal_analytics_python3-1.1.1-py3-none-any.whl.metadata (5.5 kB)\n","Collecting pydash (from fiftyone)\n","  Downloading pydash-8.0.4-py3-none-any.whl.metadata (4.5 kB)\n","Collecting fiftyone-brain<0.18,>=0.17.0 (from fiftyone)\n","  Downloading fiftyone_brain-0.17.0-py3-none-any.whl.metadata (12 kB)\n","Collecting fiftyone-db<2.0,>=0.4 (from fiftyone)\n","  Downloading fiftyone_db-1.1.7.tar.gz (7.9 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting voxel51-eta<0.14,>=0.13.0 (from fiftyone)\n","  Downloading voxel51_eta-0.13.0-py2.py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (from fiftyone) (4.10.0.84)\n","Requirement already satisfied: exceptiongroup>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (1.2.2)\n","Requirement already satisfied: h11 in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (0.14.0)\n","Collecting h2>=3.1.0 (from hypercorn>=0.13.2->fiftyone)\n","  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n","Collecting priority (from hypercorn>=0.13.2->fiftyone)\n","  Downloading priority-2.0.0-py3-none-any.whl.metadata (6.6 kB)\n","Collecting taskgroup (from hypercorn>=0.13.2->fiftyone)\n","  Downloading taskgroup-0.0.0a4-py2.py3-none-any.whl.metadata (327 bytes)\n","Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (2.0.2)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from hypercorn>=0.13.2->fiftyone) (4.12.2)\n","Collecting wsproto>=0.14.0 (from hypercorn>=0.13.2->fiftyone)\n","  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3->fiftyone) (3.0.2)\n","Collecting choreographer>=0.99.6 (from kaleido!=0.2.1.post1->fiftyone)\n","  Downloading choreographer-0.99.6-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: async-timeout in /usr/local/lib/python3.10/dist-packages (from kaleido!=0.2.1.post1->fiftyone) (4.0.3)\n","INFO: pip is looking at multiple versions of motor to determine which version is compatible with other requirements. This could take a while.\n","Collecting motor>=2.5 (from fiftyone)\n","  Downloading motor-3.5.3-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=4.14->fiftyone) (9.0.0)\n","Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<4.9,>=3.12->fiftyone)\n","  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette>=0.24.0->fiftyone) (3.7.1)\n","Requirement already satisfied: httpx>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from universal-analytics-python3<2,>=1.0.1->fiftyone) (0.27.2)\n","Collecting dill (from voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (1.0.0)\n","Requirement already satisfied: glob2 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (0.7)\n","Collecting jsonlines (from voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n","Collecting py7zr (from voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading py7zr-0.22.0-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (2.8.2)\n","Collecting rarfile (from voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading rarfile-4.2-py3-none-any.whl.metadata (4.4 kB)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (2.32.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (1.16.0)\n","Collecting sortedcontainers (from voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (5.2)\n","Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from voxel51-eta<0.14,>=0.13.0->fiftyone) (2.2.3)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->fiftyone) (2.6)\n","Collecting botocore<1.36.0,>=1.35.62 (from boto3->fiftyone)\n","  Downloading botocore-1.35.62-py3-none-any.whl.metadata (5.7 kB)\n","Collecting jmespath<2.0.0,>=0.7.1 (from boto3->fiftyone)\n","  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3->fiftyone)\n","  Downloading s3transfer-0.10.3-py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->fiftyone) (1.16.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->fiftyone) (0.2.13)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.3.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (4.54.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (1.4.7)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fiftyone) (3.2.0)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->fiftyone) (2024.2)\n","Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (3.4.2)\n","Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2.36.0)\n","Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (2024.9.20)\n","Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->fiftyone) (0.4)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fiftyone) (3.5.0)\n","Collecting graphql-core<3.4.0,>=3.2.0 (from strawberry-graphql->fiftyone)\n","  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette>=0.24.0->fiftyone) (1.3.1)\n","Collecting hyperframe<7,>=6.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n","  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n","Collecting hpack<5,>=4.0 (from h2>=3.1.0->hypercorn>=0.13.2->fiftyone)\n","  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (2024.8.30)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.10.0->universal-analytics-python3<2,>=1.0.1->fiftyone) (1.0.6)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonlines->voxel51-eta<0.14,>=0.13.0->fiftyone) (24.2.0)\n","Collecting texttable (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n","Collecting pycryptodomex>=3.16.0 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Collecting pyzstd>=0.15.9 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.4 kB)\n","Collecting pyppmd<1.2.0,>=1.1.0 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n","Collecting pybcj<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting multivolumefile>=0.2.3 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading multivolumefile-0.2.3-py3-none-any.whl.metadata (6.3 kB)\n","Collecting inflate64<1.1.0,>=1.0.0 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n","Collecting brotli>=1.1.0 (from py7zr->voxel51-eta<0.14,>=0.13.0->fiftyone)\n","  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.5 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->voxel51-eta<0.14,>=0.13.0->fiftyone) (3.4.0)\n","Downloading fiftyone-1.0.2-py3-none-any.whl (10.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mongoengine-0.24.2-py3-none-any.whl (108 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.9/108.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dacite-1.7.0-py3-none-any.whl (12 kB)\n","Downloading fiftyone_brain-0.17.0-py3-none-any.whl (98 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hypercorn-0.17.3-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading kaleido-0.4.1-py3-none-any.whl (3.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading motor-3.5.3-py3-none-any.whl (74 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.7/74.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sse_starlette-0.10.3-py3-none-any.whl (8.0 kB)\n","Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n","Downloading starlette-0.41.2-py3-none-any.whl (73 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.3/73.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading universal_analytics_python3-1.1.1-py3-none-any.whl (10 kB)\n","Downloading voxel51_eta-0.13.0-py2.py3-none-any.whl (943 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m943.1/943.1 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n","Downloading argcomplete-3.5.1-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading boto3-1.35.62-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m676.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pprintpp-0.4.0-py2.py3-none-any.whl (16 kB)\n","Downloading pydash-8.0.4-py3-none-any.whl (101 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.9/101.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n","Downloading strawberry_graphql-0.248.1-py3-none-any.whl (297 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n","Downloading botocore-1.35.62-py3-none-any.whl (12.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading choreographer-0.99.6-py3-none-any.whl (20 kB)\n","Downloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Downloading s3transfer-0.10.3-py3-none-any.whl (82 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.6/82.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n","Downloading priority-2.0.0-py3-none-any.whl (8.9 kB)\n","Downloading py7zr-0.22.0-py3-none-any.whl (67 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading rarfile-4.2-py3-none-any.whl (29 kB)\n","Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n","Downloading taskgroup-0.0.0a4-py2.py3-none-any.whl (9.1 kB)\n","Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n","Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n","Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n","Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyzstd-0.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (413 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n","Building wheels for collected packages: fiftyone-db\n","  Building wheel for fiftyone-db (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fiftyone-db: filename=fiftyone_db-1.1.7-py3-none-manylinux1_x86_64.whl size=42156157 sha256=5ff9b2cc702a550223ef7a337a41be0b444ce557a68258f6618cd48965a88888\n","  Stored in directory: /root/.cache/pip/wheels/ce/8b/e8/4f778229cacacc9c4e9871a9e0d7bce98fc99b8c01af0ea669\n","Successfully built fiftyone-db\n","Installing collected packages: texttable, sseclient-py, sortedcontainers, pprintpp, brotli, xmltodict, wsproto, taskgroup, retrying, rarfile, pyzstd, pyppmd, pydash, pycryptodomex, pybcj, priority, multivolumefile, jsonlines, jmespath, inflate64, hyperframe, hpack, graphql-core, ftfy, fiftyone-db, dnspython, dill, dacite, choreographer, argcomplete, aiofiles, strawberry-graphql, starlette, pymongo, py7zr, kaleido, h2, botocore, voxel51-eta, universal-analytics-python3, sse-starlette, s3transfer, motor, mongoengine, hypercorn, fiftyone-brain, boto3, fiftyone\n","Successfully installed aiofiles-24.1.0 argcomplete-3.5.1 boto3-1.35.62 botocore-1.35.62 brotli-1.1.0 choreographer-0.99.6 dacite-1.7.0 dill-0.3.9 dnspython-2.7.0 fiftyone-1.0.2 fiftyone-brain-0.17.0 fiftyone-db-1.1.7 ftfy-6.3.1 graphql-core-3.2.5 h2-4.1.0 hpack-4.0.0 hypercorn-0.17.3 hyperframe-6.0.1 inflate64-1.0.0 jmespath-1.0.1 jsonlines-4.0.0 kaleido-0.4.1 mongoengine-0.24.2 motor-3.5.3 multivolumefile-0.2.3 pprintpp-0.4.0 priority-2.0.0 py7zr-0.22.0 pybcj-1.0.2 pycryptodomex-3.21.0 pydash-8.0.4 pymongo-4.8.0 pyppmd-1.1.0 pyzstd-0.16.2 rarfile-4.2 retrying-1.3.4 s3transfer-0.10.3 sortedcontainers-2.4.0 sse-starlette-0.10.3 sseclient-py-1.8.0 starlette-0.41.2 strawberry-graphql-0.248.1 taskgroup-0.0.0a4 texttable-1.7.0 universal-analytics-python3-1.1.1 voxel51-eta-0.13.0 wsproto-1.2.0 xmltodict-0.14.2\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install fiftyone"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1731697350578,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"},"user_tz":-120},"id":"_fzxugFjqc2T","outputId":"5425b976-3347-4157-88a7-191de08658bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}],"source":["!ls"]},{"cell_type":"markdown","metadata":{"id":"UauKNtFRqydu"},"source":["### Import libs"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qSbY3Amlqpkh","executionInfo":{"status":"ok","timestamp":1731697351650,"user_tz":-120,"elapsed":1082,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["from __future__ import division\n","from __future__ import print_function\n","from __future__ import absolute_import\n","import random\n","import pprint\n","import sys\n","import time\n","import numpy as np\n","from optparse import OptionParser\n","import pickle\n","import math\n","import cv2\n","import copy\n","from matplotlib import pyplot as plt\n","import tensorflow as tf\n","import pandas as pd\n","import os\n","\n","from sklearn.metrics import average_precision_score\n","\n","from keras import backend as K\n","from keras.optimizers import Adam, SGD, RMSprop\n","from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n","from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n","#from keras.engine.topology import get_source_inputs\n","#from keras.utils import layer_utils\n","#from keras.utils.data_utils import get_file\n","#from keras.objectives import categorical_crossentropy\n","\n","from keras.models import Model\n","#from keras.utils import generic_utils\n","#from keras.engine import Layer, InputSpec\n","from keras import initializers, regularizers"]},{"cell_type":"markdown","metadata":{"id":"WrH5i5mmrDWY"},"source":["#### Config setting"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"DvJm0FFRsyVu","executionInfo":{"status":"ok","timestamp":1731697351650,"user_tz":-120,"elapsed":15,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["class Config:\n","\n","\tdef __init__(self):\n","\n","\t\t# Print the process or not\n","\t\tself.verbose = True\n","\n","\t\t# Name of base network\n","\t\tself.network = 'vgg'\n","\n","\t\t# Setting for data augmentation\n","\t\tself.use_horizontal_flips = False\n","\t\tself.use_vertical_flips = False\n","\t\tself.rot_90 = False\n","\n","\t\t# Anchor box scales\n","    # Note that if im_size is smaller, anchor_box_scales should be scaled\n","    # # Original anchor_box_scales in the paper is [128, 256, 512]\n","\t\t# self.anchor_box_scales = [64, 128, 256]\n","\t\t# # Anchor box ratios\n","\t\t# self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n","\t\tself.anchor_box_scales = [64, 128, 256]  # 3 scales\n","\t\tself.anchor_box_ratios = [[1, 1], [1, 2], [2, 1], [1, 3], [3, 1], [2, 3]]  # 6 ratios\n","\t\t# Size to resize the smallest side of the image\n","\t\t# Original setting in paper is 600. Set to 300 in here to save training time\n","\t\tself.im_size = 600\n","\n","\t\t# image channel-wise mean to subtract\n","\t\tself.img_channel_mean = [103.939, 116.779, 123.68]\n","\t\tself.img_scaling_factor = 1.0\n","\n","\t\t# number of ROIs at once\n","\t\tself.num_rois = 4\n","\n","\t\t# stride at the RPN (this depends on the network configuration)\n","\t\tself.rpn_stride = 16\n","\n","\t\tself.balanced_classes = False\n","\n","\t\t# scaling the stdev\n","\t\tself.std_scaling = 4.0\n","\t\tself.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n","\n","\t\t# overlaps for RPN\n","\t\tself.rpn_min_overlap = 0.1\n","\t\tself.rpn_max_overlap = 0.5\n","\n","\t\t# overlaps for classifier ROIs\n","\t\tself.classifier_min_overlap = 0.1\n","\t\tself.classifier_max_overlap = 0.5\n","\n","\t\t# placeholder for the class mapping, automatically generated by the parser\n","\t\tself.class_mapping = None\n","\n","\t\tself.model_path = None"]},{"cell_type":"markdown","metadata":{"id":"o0bIjlycyR9_"},"source":["#### Parser the data from annotation file"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"vc89E9uAydTX","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":16,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def get_data(input_path, limit=None):\n","    \"\"\"Parse the data from annotation file\n","\n","    Args:\n","        input_path: annotation file path\n","        limit: optional limit to load only a certain number of images\n","\n","    Returns:\n","        all_data: list(filepath, width, height, list(bboxes))\n","        classes_count: dict{key:class_name, value:count_num}\n","            e.g. {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745}\n","        class_mapping: dict{key:class_name, value: idx}\n","            e.g. {'Car': 0, 'Mobile phone': 1, 'Person': 2}\n","    \"\"\"\n","    found_bg = False\n","    all_imgs = {}\n","    classes_count = {}\n","    class_mapping = {}\n","    visualise = True\n","    i = 1\n","\n","    with open(input_path, 'r') as f:\n","        print('Parsing annotation files')\n","\n","        for line in f:\n","            # If a limit is set and we've reached it, stop parsing\n","            if limit and len(all_imgs) >= limit:\n","                break\n","\n","            # Print process\n","            sys.stdout.write('\\r'+'idx=' + str(i))\n","            i += 1\n","\n","            line_split = line.strip().split(',')\n","\n","            # Parse the line: (path_filename, x1, y1, x2, y2, class_name)\n","            (filename, x1, y1, x2, y2, class_name) = line_split\n","\n","            # Update class counts\n","            if class_name not in classes_count:\n","                classes_count[class_name] = 1\n","            else:\n","                classes_count[class_name] += 1\n","\n","            # Update class mapping\n","            if class_name not in class_mapping:\n","                if class_name == 'bg' and found_bg == False:\n","                    print('Found class name with special name bg. Will be treated as a background region.')\n","                    found_bg = True\n","                class_mapping[class_name] = len(class_mapping)\n","\n","            # Add image if not already in all_imgs\n","            if filename not in all_imgs:\n","                all_imgs[filename] = {}\n","                img = cv2.imread(filename)\n","                if img is None:\n","                    print(f\"Warning: {filename} could not be loaded.\")\n","                    continue  # Skip if the image can't be loaded\n","\n","                (rows, cols) = img.shape[:2]\n","                all_imgs[filename]['filepath'] = filename\n","                all_imgs[filename]['width'] = cols\n","                all_imgs[filename]['height'] = rows\n","                all_imgs[filename]['bboxes'] = []\n","\n","            # Append the bounding box for this image\n","            all_imgs[filename]['bboxes'].append({\n","                'class': class_name,\n","                'x1': int(x1),\n","                'x2': int(x2),\n","                'y1': int(y1),\n","                'y2': int(y2)\n","            })\n","\n","    all_data = list(all_imgs.values())\n","\n","    # Ensure 'bg' class is included if not found in annotations\n","    if 'bg' not in classes_count:\n","        classes_count['bg'] = 0\n","    if 'bg' not in class_mapping:\n","        class_mapping['bg'] = len(class_mapping)\n","\n","    # Ensure background class is last in the mapping\n","    if found_bg:\n","        if class_mapping['bg'] != len(class_mapping) - 1:\n","            key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping) - 1][0]\n","            val_to_switch = class_mapping['bg']\n","            class_mapping['bg'] = len(class_mapping) - 1\n","            class_mapping[key_to_switch] = val_to_switch\n","\n","    return all_data, classes_count, class_mapping"]},{"cell_type":"markdown","metadata":{"id":"oFvqGs4acGWl"},"source":["#### Define ROI Pooling Convolutional Layer"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"6l32Q85kcMpB","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":15,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["from tensorflow.keras.layers import Layer\n","\n","class RoiPoolingConv(Layer):\n","    '''ROI pooling layer for 2D inputs.\n","    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n","    K. He, X. Zhang, S. Ren, J. Sun\n","    # Arguments\n","        pool_size: int\n","            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n","        num_rois: number of regions of interest to be used\n","    # Input shape\n","        list of two 4D tensors [X_img,X_roi] with shape:\n","        X_img:\n","        `(1, rows, cols, channels)`\n","        X_roi:\n","        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n","    # Output shape\n","        3D tensor with shape:\n","        `(1, num_rois, channels, pool_size, pool_size)`\n","    '''\n","    def __init__(self, pool_size, num_rois, **kwargs):\n","\n","        self.pool_size = pool_size\n","        self.num_rois = num_rois\n","\n","        super(RoiPoolingConv, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.nb_channels = input_shape[0][3]\n","\n","    def compute_output_shape(self, input_shape):\n","        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n","\n","    def call(self, x, mask=None):\n","\n","        assert(len(x) == 2)\n","\n","        # x[0] is image with shape (rows, cols, channels)\n","        img = x[0]\n","\n","        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n","        rois = x[1]\n","\n","        input_shape = K.shape(img)\n","\n","        outputs = []\n","\n","        for roi_idx in range(self.num_rois):\n","\n","            x = rois[0, roi_idx, 0]\n","            y = rois[0, roi_idx, 1]\n","            w = rois[0, roi_idx, 2]\n","            h = rois[0, roi_idx, 3]\n","\n","            x = K.cast(x, 'int32')\n","            y = K.cast(y, 'int32')\n","            w = K.cast(w, 'int32')\n","            h = K.cast(h, 'int32')\n","\n","            # Resized roi of the image to pooling size (7x7)\n","            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n","            outputs.append(rs)\n","\n","\n","        final_output = K.concatenate(outputs, axis=0)\n","\n","        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n","        # Might be (1, 4, 7, 7, 3)\n","        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n","\n","        # permute_dimensions is similar to transpose\n","        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n","\n","        return final_output\n","\n","\n","    def get_config(self):\n","        config = {'pool_size': self.pool_size,\n","                  'num_rois': self.num_rois}\n","        base_config = super(RoiPoolingConv, self).get_config()\n","        return dict(list(base_config.items()) + list(config.items()))"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.layers import Layer\n","\n","class RoiPoolingConv(Layer):\n","    def __init__(self, pool_size, num_rois, **kwargs):\n","        self.pool_size = pool_size\n","        self.num_rois = num_rois\n","        super(RoiPoolingConv, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.nb_channels = input_shape[0][-1]\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0][0], self.num_rois, self.pool_size, self.pool_size, self.nb_channels)\n","\n","    @tf.function\n","    def call(self, x, mask=None):\n","        img = x[0]  # Shape: (batch_size, height, width, channels)\n","        rois = x[1]  # Shape: (batch_size, num_rois, 4)\n","\n","        batch_size = tf.shape(img)[0]\n","\n","        # Print shapes to confirm consistency in debugging\n","        # print(\"Shape of img:\", tf.shape(img))\n","        # print(\"Shape of rois:\", tf.shape(rois))\n","        # print(\"batch_size:\", tf.range(batch_size))\n","        # print(\"num_rois:\", tf.range(self.num_rois))\n","\n","        # Ensure `box_indices` and `rois_flat` align correctly\n","        box_indices = tf.repeat(tf.range(batch_size), repeats=self.num_rois)\n","        # print(\"Shape of box_indices:\", tf.shape(box_indices))\n","        rois_flat = tf.reshape(rois, (-1, 4))\n","        # print(\"Shape of rois_flat:\", tf.shape(rois_flat))\n","\n","        # # Use tf.debugging.assert_equal for the shape check instead of Python condition\n","        tf.debugging.assert_equal(\n","            tf.shape(box_indices), tf.shape(rois_flat)[0],\n","            message=\"Mismatch between box_indices and rois_flat dimensions\"\n","        )\n","\n","        # Apply `crop_and_resize`\n","        try:\n","            pooled_rois = tf.image.crop_and_resize(\n","                img, boxes=rois_flat, box_indices=box_indices, crop_size=(self.pool_size, self.pool_size)\n","            )\n","            # tf.print(\"Shape of pooled_rois after crop_and_resize:\", tf.shape(pooled_rois))\n","        except Exception as e:\n","            tf.print(\"Exception in crop_and_resize:\", e)\n","            raise e\n","\n","        # Reshape to match the desired output format\n","        final_output = tf.reshape(\n","            pooled_rois, (batch_size, self.num_rois, self.pool_size, self.pool_size, self.nb_channels)\n","        )\n","        # tf.print(\"Shape of final output:\", tf.shape(final_output))\n","\n","        return final_output\n","\n","    def get_config(self):\n","        config = {'pool_size': self.pool_size, 'num_rois': self.num_rois}\n","        base_config = super(RoiPoolingConv, self).get_config()\n","        return {**base_config, **config}"],"metadata":{"id":"FUNFxtBHZ6qi","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":15,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mf2taA29RFNs"},"source":["#### Vgg-16 model"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"WaBQfl4XRJY3","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":15,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def get_img_output_length(width, height):\n","    def get_output_length(input_length):\n","        return input_length//16\n","\n","    return get_output_length(width), get_output_length(height)\n","\n","def nn_base(input_tensor=None, trainable=False):\n","\n","\n","    input_shape = (None, None, 3)\n","\n","    if input_tensor is None:\n","        img_input = Input(shape=input_shape)\n","    else:\n","        if not K.is_keras_tensor(input_tensor):\n","            img_input = Input(tensor=input_tensor, shape=input_shape)\n","        else:\n","            img_input = input_tensor\n","\n","    bn_axis = 3\n","\n","    # Block 1\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n","    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n","\n","    # Block 2\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n","    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n","\n","    # Block 3\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n","    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n","\n","    # Block 4\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n","    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n","\n","    # Block 5\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n","    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n","    # x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n","\n","    return x"]},{"cell_type":"markdown","metadata":{"id":"xcOi5MIMVJpU"},"source":["####  RPN layer"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"gsuV21vpRczQ","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":14,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def rpn_layer(base_layers, num_anchors):\n","    \"\"\"Create a rpn layer\n","        Step1: Pass through the feature map from base layer to a 3x3 512 channels convolutional layer\n","                Keep the padding 'same' to preserve the feature map's size\n","        Step2: Pass the step1 to two (1,1) convolutional layer to replace the fully connected layer\n","                classification layer: num_anchors (9 in here) channels for 0, 1 sigmoid activation output\n","                regression layer: num_anchors*4 (36 in here) channels for computing the regression of bboxes with linear activation\n","    Args:\n","        base_layers: vgg in here\n","        num_anchors: 9 in here\n","\n","    Returns:\n","        [x_class, x_regr, base_layers]\n","        x_class: classification for whether it's an object\n","        x_regr: bboxes regression\n","        base_layers: vgg in here\n","    \"\"\"\n","    x = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n","\n","    # x_class = Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n","    # x_regr = Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n","    # x_class = Conv2D(18, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n","    # x_regr = Conv2D(18 * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n","    x_class = Conv2D(2 * num_anchors, (1, 1), activation='sigmoid', name='rpn_out_class')(x)\n","    print(f\"x_class shape (classification output): {x_class.shape}\")\n","\n","    x_regr = Conv2D(4 * num_anchors, (1, 1), activation='linear', name='rpn_out_regress')(x)\n","    print(f\"x_regr shape (regression output): {x_regr.shape}\")\n","\n","\n","    return [x_class, x_regr, base_layers]"]},{"cell_type":"markdown","metadata":{"id":"0fBt9xNFWsKS"},"source":["####  Classifier layer"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"0PKSPLRLWwMz","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":14,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 4):\n","    \"\"\"Create a classifier layer\n","\n","    Args:\n","        base_layers: vgg\n","        input_rois: `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n","        num_rois: number of rois to be processed in one time (4 in here)\n","\n","    Returns:\n","        list(out_class, out_regr)\n","        out_class: classifier layer output\n","        out_regr: regression layer output\n","    \"\"\"\n","\n","    input_shape = (num_rois,7,7,512)\n","\n","    pooling_regions = 7\n","\n","    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n","    # num_rois (4) 7x7 roi pooling\n","    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n","\n","    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n","    out = TimeDistributed(Flatten(name='flatten'))(out_roi_pool)\n","    out = TimeDistributed(Dense(4096, activation='relu', name='fc1'))(out)\n","    out = TimeDistributed(Dropout(0.5))(out)\n","    out = TimeDistributed(Dense(4096, activation='relu', name='fc2'))(out)\n","    out = TimeDistributed(Dropout(0.5))(out)\n","    print(f\"Using {nb_classes} classes for classifier and {4 * (nb_classes - 1)} outputs for regression\")\n","\n","    # There are two output layer\n","    # out_class: softmax acivation function for classify the class name of the object\n","    # out_regr: linear activation function for bboxes coordinates regression\n","    out_class = TimeDistributed(Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n","    # note: no regression target for bg class\n","    out_regr = TimeDistributed(Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n","\n","    return [out_class, out_regr]"]},{"cell_type":"markdown","metadata":{"id":"WMev3UMadCzJ"},"source":["#### Calculate IoU (Intersection of Union)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"Jy5iIBYgdCJD","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":14,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def union(au, bu, area_intersection):\n","\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n","\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n","\tarea_union = area_a + area_b - area_intersection\n","\treturn area_union\n","\n","\n","def intersection(ai, bi):\n","\tx = max(ai[0], bi[0])\n","\ty = max(ai[1], bi[1])\n","\tw = min(ai[2], bi[2]) - x\n","\th = min(ai[3], bi[3]) - y\n","\tif w < 0 or h < 0:\n","\t\treturn 0\n","\treturn w*h\n","\n","\n","def iou(a, b):\n","\t# a and b should be (x1,y1,x2,y2)\n","\n","\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n","\t\treturn 0.0\n","\n","\tarea_i = intersection(a, b)\n","\tarea_u = union(a, b, area_i)\n","\n","\treturn float(area_i) / float(area_u + 1e-6)"]},{"cell_type":"markdown","metadata":{"id":"rcRlzqZudKkd"},"source":["#### Calculate the rpn for all anchors of all images"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"daPsCZtrdK3S","executionInfo":{"status":"ok","timestamp":1731697351651,"user_tz":-120,"elapsed":13,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["import numpy as np\n","import random\n","\n","def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n","    valid_anchors = []\n","    \"\"\"(Important part!) Calculate the rpn for all anchors\n","    Args:\n","\n","        C: config\n","        img_data: augmented image data\n","        width: original image width (e.g. 600)\n","        height: original image height (e.g. 800)\n","        resized_width: resized image width according to C.im_size (e.g. 300)\n","        resized_height: resized image height according to C.im_size (e.g. 400)\n","        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n","\n","    Returns:\n","        y_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n","        y_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n","        num_pos: number of positive anchor boxes\n","    \"\"\"\n","    downscale = float(C.rpn_stride)\n","    anchor_sizes = C.anchor_box_scales   # 128, 256, 512\n","    anchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n","    num_anchors = len(anchor_sizes) * len(anchor_ratios)  # 3x3=9\n","    # print(f\"Config: rpn_stride={C.rpn_stride}, num_anchors={num_anchors}, anchor_sizes={anchor_sizes}, anchor_ratios={anchor_ratios}\")\n","    # print(f\"Calculated num_anchors: {num_anchors}\")\n","\n","    # Calculate the output map size based on the network architecture\n","    (output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n","    # print(f\"Calculated output map size: width={output_width}, height={output_height}\")\n","\n","    # Initialize output variables\n","    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n","    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n","    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n","    # print(f\"Initial shapes - y_rpn_overlap: {y_rpn_overlap.shape}, y_rpn_regr: {y_rpn_regr.shape}\")\n","\n","    num_bboxes = len(img_data['bboxes'])\n","\n","    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n","    best_anchor_for_bbox = -1 * np.ones((num_bboxes, 4)).astype(int)\n","    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n","    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n","    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n","\n","    # Get the ground truth box coordinates, and resize to account for image resizing\n","    gta = np.zeros((num_bboxes, 4))\n","    for bbox_num, bbox in enumerate(img_data['bboxes']):\n","        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n","        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n","        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n","        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n","        # print(f\"BBox {bbox_num}: Resized coordinates={gta[bbox_num]}, Class={bbox['class']}\")\n","\n","    for anchor_size_idx in range(len(anchor_sizes)):\n","        for anchor_ratio_idx in range(len(anchor_ratios)):\n","            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n","            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n","            # print(f\"Anchor configuration: size={anchor_sizes[anchor_size_idx]}, ratio=({ratio_x}, {ratio_y}), width={anchor_x}, height={anchor_y}\")\n","            # anchor_size = anchor_sizes[anchor_size_idx]  # Define anchor_size correctly\n","            # ratio_x, ratio_y = anchor_ratios[anchor_ratio_idx]  # Define ratio_x and ratio_y\n","\n","            # anchor_x = anchor_size * ratio_x\n","            # anchor_y = anchor_size * ratio_y\n","            # print(f\"Anchor configuration: size={anchor_size}, ratio=({ratio_x}, {ratio_y}), width={anchor_x}, height={anchor_y}\")\n","\n","            for ix in range(output_width):\n","                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n","                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\n","                # Adjust anchors that exceed image boundaries\n","                x1_anc = max(0, x1_anc)\n","                x2_anc = min(resized_width, x2_anc)\n","                print(f\"Anchor: size_idx={anchor_size_idx}, ratio_idx={anchor_ratio_idx}, anchor_x={anchor_x}, anchor_y={anchor_y}\")\n","\n","                if x1_anc < 0 or x2_anc > resized_width:\n","                    print(f\"Skipping x-anchor ({ix}) out of bounds: x1_anc={x1_anc}, x2_anc={x2_anc}\")\n","                    continue\n","\n","                for jy in range(output_height):\n","                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n","                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n","\n","                    if y1_anc < 0 or y2_anc > resized_height:\n","                        continue\n","                    # print(\" printing \",x1_anc,y1_anc,x2_anc, y2_anc)\n","                    # if 0 <= x1_anc < resized_width and 0 <= x2_anc <= resized_width and 0 <= y1_anc < resized_height and 0 <= y2_anc <= resized_height:\n","                    valid_anchors.append([x1_anc, y1_anc, x2_anc, y2_anc])\n","                    print(\"valid anchors\",valid_anchors)\n","\n","                    bbox_type = 'neg'\n","                    best_iou_for_loc = 0.0\n","\n","                    for bbox_num in range(num_bboxes):\n","                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],\n","                                       [x1_anc, y1_anc, x2_anc, y2_anc])\n","\n","                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n","                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n","                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n","                            cxa = (x1_anc + x2_anc) / 2.0\n","                            cya = (y1_anc + y2_anc) / 2.0\n","\n","                            tx = (cx - cxa) / (x2_anc - x1_anc)\n","                            ty = (cy - cya) / (y2_anc - y1_anc)\n","                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n","                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n","\n","                        if img_data['bboxes'][bbox_num]['class'] != 'bg':\n","                            if curr_iou > best_iou_for_bbox[bbox_num]:\n","                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n","                                best_iou_for_bbox[bbox_num] = curr_iou\n","                                best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n","                                best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n","\n","                            if curr_iou > C.rpn_max_overlap:\n","                                bbox_type = 'pos'\n","                                num_anchors_for_bbox[bbox_num] += 1\n","                                if curr_iou > best_iou_for_loc:\n","                                    best_iou_for_loc = curr_iou\n","                                    best_regr = (tx, ty, tw, th)\n","\n","                            if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n","                                if bbox_type != 'pos':\n","                                    bbox_type = 'neutral'\n","\n","                    if bbox_type == 'neg':\n","                        y_is_box_valid[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 1\n","                        y_rpn_overlap[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 0\n","                    elif bbox_type == 'neutral':\n","                        y_is_box_valid[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 0\n","                        y_rpn_overlap[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 0\n","                    elif bbox_type == 'pos':\n","                        y_is_box_valid[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 1\n","                        y_rpn_overlap[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 1\n","                        start = 4 * (anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx)\n","                        y_rpn_regr[jy, ix, start:start+4] = best_regr\n","\n","    # Ensure every bbox has at least one positive RPN region\n","    for idx in range(num_anchors_for_bbox.shape[0]):\n","        if num_anchors_for_bbox[idx] == 0:\n","            if best_anchor_for_bbox[idx, 0] == -1:\n","                continue\n","            y_is_box_valid[best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1],\n","                           best_anchor_for_bbox[idx, 2] + len(anchor_ratios) * best_anchor_for_bbox[idx, 3]] = 1\n","            y_rpn_overlap[best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1],\n","                          best_anchor_for_bbox[idx, 2] + len(anchor_ratios) * best_anchor_for_bbox[idx, 3]] = 1\n","            start = 4 * (best_anchor_for_bbox[idx, 2] + len(anchor_ratios) * best_anchor_for_bbox[idx, 3])\n","            y_rpn_regr[best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], start:start + 4] = best_dx_for_bbox[idx, :]\n","\n","    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n","    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n","\n","    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n","    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n","\n","    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n","    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n","\n","    pos_locs = np.where(y_rpn_overlap[0] == 1)\n","    neg_locs = np.where(y_rpn_overlap[0] == 0)\n","\n","    num_pos = len(pos_locs[0])\n","\n","    num_regions = 256\n","\n","    if len(pos_locs[0]) > num_regions / 2:\n","        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions // 2)\n","        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n","        num_pos = num_regions // 2\n","\n","    if len(neg_locs[0]) + num_pos > num_regions:\n","        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n","        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n","\n","\n","\n","\n","\n","    # Concatenate for classification (y_rpn_cls) along the anchor dimension.\n","    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)  # Results in (batch_size, 36, height, width)\n","\n","\n","    # Transpose both to expected (batch_size, height, width, channels) shape if needed\n","    y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))  # Should now be (batch_size, height, width, 36)\n","    y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))  # Should now be (batch_size, height, width, 72)\n","\n","\n","    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos, valid_anchors"]},{"cell_type":"code","source":["import numpy as np\n","import random\n","\n","def calc_rpn(C, img_data, width, height, resized_width, resized_height, img_length_calc_function):\n","    valid_anchors = []\n","    \"\"\"(Important part!) Calculate the rpn for all anchors\n","    Args:\n","\n","        C: config\n","        img_data: augmented image data\n","        width: original image width (e.g. 600)\n","        height: original image height (e.g. 800)\n","        resized_width: resized image width according to C.im_size (e.g. 300)\n","        resized_height: resized image height according to C.im_size (e.g. 400)\n","        img_length_calc_function: function to calculate final layer's feature map (of base model) size according to input image size\n","\n","    Returns:\n","        y_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n","        y_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n","        num_pos: number of positive anchor boxes\n","    \"\"\"\n","    downscale = float(C.rpn_stride)\n","    anchor_sizes = C.anchor_box_scales   # 128, 256, 512\n","    anchor_ratios = C.anchor_box_ratios  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n","    num_anchors = len(anchor_sizes) * len(anchor_ratios)  # 3x3=9\n","    # print(f\"Config: rpn_stride={C.rpn_stride}, num_anchors={num_anchors}, anchor_sizes={anchor_sizes}, anchor_ratios={anchor_ratios}\")\n","    # print(f\"Calculated num_anchors: {num_anchors}\")\n","\n","    # Calculate the output map size based on the network architecture\n","    (output_width, output_height) = img_length_calc_function(resized_width, resized_height)\n","    # print(f\"Calculated output map size: width={output_width}, height={output_height}\")\n","\n","    # Initialize output variables\n","    y_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n","    y_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n","    y_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n","    # print(f\"Initial shapes - y_rpn_overlap: {y_rpn_overlap.shape}, y_rpn_regr: {y_rpn_regr.shape}\")\n","\n","    num_bboxes = len(img_data['bboxes'])\n","\n","    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n","    best_anchor_for_bbox = -1 * np.ones((num_bboxes, 4)).astype(int)\n","    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n","    best_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n","    best_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n","\n","    # Get the ground truth box coordinates, and resize to account for image resizing\n","    gta = np.zeros((num_bboxes, 4))\n","    for bbox_num, bbox in enumerate(img_data['bboxes']):\n","        gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n","        gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n","        gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n","        gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n","        # print(f\"BBox {bbox_num}: Resized coordinates={gta[bbox_num]}, Class={bbox['class']}\")\n","\n","    for anchor_size_idx in range(len(anchor_sizes)):\n","        for anchor_ratio_idx in range(len(anchor_ratios)):\n","            anchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n","            anchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\n","            # print(f\"Anchor configuration: size={anchor_sizes[anchor_size_idx]}, ratio=({ratio_x}, {ratio_y}), width={anchor_x}, height={anchor_y}\")\n","            # anchor_size = anchor_sizes[anchor_size_idx]  # Define anchor_size correctly\n","            # ratio_x, ratio_y = anchor_ratios[anchor_ratio_idx]  # Define ratio_x and ratio_y\n","\n","            # anchor_x = anchor_size * ratio_x\n","            # anchor_y = anchor_size * ratio_y\n","            # print(f\"Anchor configuration: size={anchor_size}, ratio=({ratio_x}, {ratio_y}), width={anchor_x}, height={anchor_y}\")\n","\n","            for ix in range(output_width):\n","                x1_anc = downscale * (ix + 0.5) - anchor_x / 2\n","                x2_anc = downscale * (ix + 0.5) + anchor_x / 2\n","                # Adjust anchors that exceed image boundaries\n","                x1_anc = max(0, x1_anc)\n","                x2_anc = min(resized_width, x2_anc)\n","\n","                if x1_anc < 0 or x2_anc > resized_width:\n","                    print(f\"Skipping x-anchor ({ix}) out of bounds: x1_anc={x1_anc}, x2_anc={x2_anc}\")\n","                    continue\n","\n","                for jy in range(output_height):\n","                    y1_anc = downscale * (jy + 0.5) - anchor_y / 2\n","                    y2_anc = downscale * (jy + 0.5) + anchor_y / 2\n","\n","                    if y1_anc < 0 or y2_anc > resized_height:\n","                        continue\n","                    # print(\" printing \",x1_anc,y1_anc,x2_anc, y2_anc)\n","                    # if 0 <= x1_anc < resized_width and 0 <= x2_anc <= resized_width and 0 <= y1_anc < resized_height and 0 <= y2_anc <= resized_height:\n","                    valid_anchors.append([x1_anc, y1_anc, x2_anc, y2_anc])\n","                    # print(\"valid anchors\",valid_anchors)\n","\n","                    bbox_type = 'neg'\n","                    best_iou_for_loc = 0.0\n","\n","                    for bbox_num in range(num_bboxes):\n","                        curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]],\n","                                       [x1_anc, y1_anc, x2_anc, y2_anc])\n","                        # print(f\"gta[bbox_num] shape: {gta[bbox_num].shape}, anchor shape: {(x1_anc, y1_anc, x2_anc, y2_anc)}\")\n","\n","                        if curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > C.rpn_max_overlap:\n","                            cx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n","                            cy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n","                            cxa = (x1_anc + x2_anc) / 2.0\n","                            cya = (y1_anc + y2_anc) / 2.0\n","\n","                            tx = (cx - cxa) / (x2_anc - x1_anc)\n","                            ty = (cy - cya) / (y2_anc - y1_anc)\n","                            tw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n","                            th = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n","\n","                        if img_data['bboxes'][bbox_num]['class'] != 'bg':\n","                            if curr_iou > best_iou_for_bbox[bbox_num]:\n","                                best_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n","                                best_iou_for_bbox[bbox_num] = curr_iou\n","                                best_x_for_bbox[bbox_num, :] = [x1_anc, x2_anc, y1_anc, y2_anc]\n","                                best_dx_for_bbox[bbox_num, :] = [tx, ty, tw, th]\n","\n","                            if curr_iou > C.rpn_max_overlap:\n","                                bbox_type = 'pos'\n","                                num_anchors_for_bbox[bbox_num] += 1\n","                                if curr_iou > best_iou_for_loc:\n","                                    best_iou_for_loc = curr_iou\n","                                    best_regr = (tx, ty, tw, th)\n","\n","                            if C.rpn_min_overlap < curr_iou < C.rpn_max_overlap:\n","                                if bbox_type != 'pos':\n","                                    bbox_type = 'neutral'\n","\n","                    if bbox_type == 'neg':\n","                        y_is_box_valid[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 1\n","                        y_rpn_overlap[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 0\n","                    elif bbox_type == 'neutral':\n","                        y_is_box_valid[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 0\n","                        y_rpn_overlap[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 0\n","                    elif bbox_type == 'pos':\n","                        y_is_box_valid[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 1\n","                        y_rpn_overlap[jy, ix, anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx] = 1\n","                        start = 4 * (anchor_ratio_idx + len(anchor_ratios) * anchor_size_idx)\n","                        y_rpn_regr[jy, ix, start:start+4] = best_regr\n","\n","    # Ensure every bbox has at least one positive RPN region\n","    for idx in range(num_anchors_for_bbox.shape[0]):\n","        if num_anchors_for_bbox[idx] == 0:\n","            if best_anchor_for_bbox[idx, 0] == -1:\n","                continue\n","            y_is_box_valid[best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1],\n","                           best_anchor_for_bbox[idx, 2] + len(anchor_ratios) * best_anchor_for_bbox[idx, 3]] = 1\n","            y_rpn_overlap[best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1],\n","                          best_anchor_for_bbox[idx, 2] + len(anchor_ratios) * best_anchor_for_bbox[idx, 3]] = 1\n","            start = 4 * (best_anchor_for_bbox[idx, 2] + len(anchor_ratios) * best_anchor_for_bbox[idx, 3])\n","            y_rpn_regr[best_anchor_for_bbox[idx, 0], best_anchor_for_bbox[idx, 1], start:start + 4] = best_dx_for_bbox[idx, :]\n","\n","    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n","    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n","\n","    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n","    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n","\n","    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n","    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n","\n","    pos_locs = np.where(y_rpn_overlap[0] == 1)\n","    neg_locs = np.where(y_rpn_overlap[0] == 0)\n","\n","    num_pos = len(pos_locs[0])\n","\n","    num_regions = 256\n","\n","    if len(pos_locs[0]) > num_regions / 2:\n","        val_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions // 2)\n","        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n","        num_pos = num_regions // 2\n","\n","    if len(neg_locs[0]) + num_pos > num_regions:\n","        val_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n","        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n","\n","    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)  # Results in (batch_size, 36, height, width)\n","\n","\n","    # Transpose both to expected (batch_size, height, width, channels) shape if needed\n","    y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))  # Should now be (batch_size, height, width, 36)\n","    y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))  # Should now be (batch_size, height, width, 72)\n","\n","\n","    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos, valid_anchors"],"metadata":{"id":"TND6C7G6tgIt","executionInfo":{"status":"ok","timestamp":1731697351652,"user_tz":-120,"elapsed":14,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qGAalfJB8zz"},"source":["#### Get new image size and augment the image"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"HKhSFbmB2RTo","executionInfo":{"status":"ok","timestamp":1731697351653,"user_tz":-120,"elapsed":15,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def get_new_img_size(width, height, min_side=300):\n","    \"\"\"\n","    Resize image while maintaining the aspect ratio.\n","\n","    Args:\n","        width: Original width of the image\n","        height: Original height of the image\n","        min_side: The minimum size to which the smaller side of the image will be resized\n","\n","    Returns:\n","        (resized_width, resized_height): Tuple of the new width and height\n","    \"\"\"\n","    if width <= height:\n","        f = float(min_side) / width\n","        resized_height = int(f * height)\n","        resized_width = min_side\n","    else:\n","        f = float(min_side) / height\n","        resized_width = int(f * width)\n","        resized_height = min_side\n","\n","    return resized_width, resized_height\n","\n","def augment(img_data, config, augment=True):\n","\tassert 'filepath' in img_data\n","\tassert 'bboxes' in img_data\n","\tassert 'width' in img_data\n","\tassert 'height' in img_data\n","\n","\timg_data_aug = copy.deepcopy(img_data)\n","\n","\timg = cv2.imread(img_data_aug['filepath'])\n","\n","\tif augment:\n","\t\trows, cols = img.shape[:2]\n","\n","\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n","\t\t\timg = cv2.flip(img, 1)\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\tx1 = bbox['x1']\n","\t\t\t\tx2 = bbox['x2']\n","\t\t\t\tbbox['x2'] = cols - x1\n","\t\t\t\tbbox['x1'] = cols - x2\n","\n","\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n","\t\t\timg = cv2.flip(img, 0)\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\ty1 = bbox['y1']\n","\t\t\t\ty2 = bbox['y2']\n","\t\t\t\tbbox['y2'] = rows - y1\n","\t\t\t\tbbox['y1'] = rows - y2\n","\n","\t\tif config.rot_90:\n","\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n","\t\t\tif angle == 270:\n","\t\t\t\timg = np.transpose(img, (1,0,2))\n","\t\t\t\timg = cv2.flip(img, 0)\n","\t\t\telif angle == 180:\n","\t\t\t\timg = cv2.flip(img, -1)\n","\t\t\telif angle == 90:\n","\t\t\t\timg = np.transpose(img, (1,0,2))\n","\t\t\t\timg = cv2.flip(img, 1)\n","\t\t\telif angle == 0:\n","\t\t\t\tpass\n","\n","\t\t\tfor bbox in img_data_aug['bboxes']:\n","\t\t\t\tx1 = bbox['x1']\n","\t\t\t\tx2 = bbox['x2']\n","\t\t\t\ty1 = bbox['y1']\n","\t\t\t\ty2 = bbox['y2']\n","\t\t\t\tif angle == 270:\n","\t\t\t\t\tbbox['x1'] = y1\n","\t\t\t\t\tbbox['x2'] = y2\n","\t\t\t\t\tbbox['y1'] = cols - x2\n","\t\t\t\t\tbbox['y2'] = cols - x1\n","\t\t\t\telif angle == 180:\n","\t\t\t\t\tbbox['x2'] = cols - x1\n","\t\t\t\t\tbbox['x1'] = cols - x2\n","\t\t\t\t\tbbox['y2'] = rows - y1\n","\t\t\t\t\tbbox['y1'] = rows - y2\n","\t\t\t\telif angle == 90:\n","\t\t\t\t\tbbox['x1'] = rows - y2\n","\t\t\t\t\tbbox['x2'] = rows - y1\n","\t\t\t\t\tbbox['y1'] = x1\n","\t\t\t\t\tbbox['y2'] = x2\n","\t\t\t\telif angle == 0:\n","\t\t\t\t\tpass\n","\n","\timg_data_aug['width'] = img.shape[1]\n","\timg_data_aug['height'] = img.shape[0]\n","\treturn img_data_aug, img"]},{"cell_type":"markdown","metadata":{"id":"0712o8CXkyh1"},"source":["#### Generate the ground_truth anchors"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"TvsEv3RIk0cF","executionInfo":{"status":"ok","timestamp":1731697351654,"user_tz":-120,"elapsed":16,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["import traceback  # Import to print full traceback for errors\n","from keras.utils import Progbar\n","import numpy as np\n","import time\n","import logging\n","\n","# Early stopping class\n","class EarlyStopping:\n","    def __init__(self, patience=5, min_delta=0):\n","        self.patience = patience  # Number of epochs with no improvement before stopping\n","        self.min_delta = min_delta  # Minimum change to qualify as an improvement\n","        self.best_loss = np.Inf  # Initialize the best loss to infinity\n","        self.counter = 0  # Counter for epochs with no improvement\n","\n","    def should_stop(self, current_loss):\n","        if current_loss < self.best_loss - self.min_delta:\n","            self.best_loss = current_loss  # Update the best loss\n","            self.counter = 0  # Reset the counter if there's improvement\n","        else:\n","            self.counter += 1  # Increment the counter if no improvement\n","        return self.counter >= self.patience  # Stop if counter exceeds patience\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"eo_pwsHcbDJd","executionInfo":{"status":"ok","timestamp":1731697351654,"user_tz":-120,"elapsed":16,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["import time\n","import logging\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","\n","# Function to resize image with padding if smaller, and cropping if larger\n","def resize_image_with_padding_or_cropping(img, target_width, target_height):\n","    original_h, original_w, _ = img.shape\n","\n","    # Check if dimensions are valid numbers\n","    if original_w is None or original_h is None:\n","        raise ValueError(f\"Invalid dimensions for image: width={original_w}, height={original_h}\")\n","\n","    # Calculate the scaling factor to maintain aspect ratio\n","    scale = min(target_width / original_w, target_height / original_h) \\\n","        if (original_w < target_width or original_h < target_height) \\\n","        else max(target_width / original_w, target_height / original_h)\n","\n","    new_w = int(original_w * scale)\n","    new_h = int(original_h * scale)\n","\n","    # Resize image with aspect ratio\n","    resized_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n","\n","    if new_w < target_width or new_h < target_height:\n","        # Padding case (image smaller than the target size)\n","        # Create a blank canvas with the target size\n","        padded_img = np.zeros((target_height, target_width, 3), dtype=np.uint8)\n","\n","        # Place the resized image at the top-left corner of the blank canvas\n","        padded_img[:new_h, :new_w] = resized_img\n","        return padded_img, \"pad\"\n","    else:\n","        # Cropping case (image larger than the target size)\n","        cropped_img = resized_img[:target_height, :target_width]\n","        return cropped_img, \"crop\"\n"]},{"cell_type":"code","source":[],"metadata":{"id":"T62JAUvFp0cv","executionInfo":{"status":"ok","timestamp":1731697351656,"user_tz":-120,"elapsed":17,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Function to pad or crop the image without resizing it\n","def resize_image_with_padding_or_cropping(img, target_width, target_height):\n","    original_h, original_w, _ = img.shape\n","\n","    #print(f\"Original image size: width={original_w}, height={original_h}\")\n","\n","    # Pad if image is smaller than target size\n","    if original_w < target_width or original_h < target_height:\n","        padded_img = np.zeros((max(original_h, target_height), max(original_w, target_width), 3), dtype=np.uint8)\n","        padded_img[:original_h, :original_w] = img\n","\n","        # Calculate the coordinates to place the padded image in the center\n","        top_left_y = (target_height - original_h) // 2 if original_h < target_height else 0\n","        top_left_x = (target_width - original_w) // 2 if original_w < target_width else 0\n","        padded_img[top_left_y:top_left_y + original_h, top_left_x:top_left_x + original_w] = img\n","\n","        # Crop to exactly match target size if needed\n","        final_img = padded_img[:target_height, :target_width]\n","        #print(f\"Padding: Created blank canvas with shape {padded_img.shape}\")\n","        #print(f\"Padding position: top_left_x={top_left_x}, top_left_y={top_left_y}\")\n","        #print(f\"Final padded image shape: {final_img.shape}\")\n","        return final_img, \"pad\"\n","\n","    # Crop if the image is larger than target size\n","    else:\n","        start_y = (original_h - target_height) // 2 if original_h > target_height else 0\n","        start_x = (original_w - target_width) // 2 if original_w > target_width else 0\n","        cropped_img = img[start_y:start_y + target_height, start_x:start_x + target_width]\n","\n","        #print(f\"Cropping: Starting crop at start_x={start_x}, start_y={start_y}\")\n","        #print(f\"Final cropped image shape: {cropped_img.shape}\")\n","        return cropped_img, \"crop\"\n"],"metadata":{"id":"Bf7lD0odp095","executionInfo":{"status":"ok","timestamp":1731697351656,"user_tz":-120,"elapsed":17,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import os\n","import logging\n","import numpy as np\n","import cv2\n","import matplotlib.pyplot as plt\n","\n","# Set up logging for better tracking\n","logging.basicConfig(level=logging.DEBUG)\n","logger = logging.getLogger(__name__)\n","\n","# Function to visualize anchors and ground-truth bounding boxes for each image\n","def draw_boxes_and_anchors(img, bboxes, anchors):\n","    \"\"\"Draw ground-truth boxes (green) and positive anchors (red) on an image.\"\"\"\n","\n","# Print or log the size of the image\n","    #height, width = img.shape[:2]\n","    #logger.info(f\"Image size: width={width}, height={height}\")\n","    #print(f\"Image size: width={width}, height={height}\")\n","    # Debugging print statements for bboxes and anchors\n","    print(f\"Checking bboxes structure: {bboxes}\")\n","    print(f\"Checking anchors structure: {anchors}\")\n","\n","    img_copy = img.copy()\n","    img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2RGB)\n","\n","    # Draw ground-truth bounding boxes (green)\n","    for bbox in bboxes:\n","        cv2.rectangle(img_copy, (int(bbox['x1']), int(bbox['y1'])), (int(bbox['x2']), int(bbox['y2'])), (0, 255, 0), 2)\n","\n","    # Draw positive anchors (red)\n","    for anchor in anchors:\n","        cv2.rectangle(img_copy, (int(anchor['x1']), int(anchor['y1'])), (int(anchor['x2']), int(anchor['y2'])), (255, 0, 0), 1)\n","\n","    # Display the image with drawn boxes\n","    plt.figure(figsize=(8, 8))\n","    plt.grid(False)\n","    plt.imshow(img_copy)\n","    plt.show()"],"metadata":{"id":"m3k83qUrwzEk","executionInfo":{"status":"ok","timestamp":1731697351657,"user_tz":-120,"elapsed":18,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["import logging\n","import numpy as np\n","import cv2\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Placeholder for failed image loads\n","def create_placeholder_image(width, height, channels=3):\n","    return np.zeros((height, width, channels), dtype=np.uint8)\n","def generate_positive_anchors(y_rpn_cls, rpn_stride, anchor_size, min_anchors=1, img_shape=(600, 600)):\n","    pos_cls = np.where(y_rpn_cls == 1)\n","\n","    # If fewer positive anchors, use placeholders centered in the image\n","    if len(pos_cls[0]) < min_anchors:\n","        img_center_x, img_center_y = img_shape[1] // 2, img_shape[0] // 2\n","        anchors = [\n","            {\n","                'x1': int(img_center_x - anchor_size // 2),\n","                'y1': int(img_center_y - anchor_size // 2),\n","                'x2': int(img_center_x + anchor_size // 2),\n","                'y2': int(img_center_y + anchor_size // 2)\n","            }\n","            for _ in range(min_anchors)\n","        ]\n","    else:\n","        anchors = [\n","            {\n","                'x1': int(pos_cls[1][i] * rpn_stride - anchor_size // 2),\n","                'y1': int(pos_cls[0][i] * rpn_stride - anchor_size // 2),\n","                'x2': int(pos_cls[1][i] * rpn_stride + anchor_size // 2),\n","                'y2': int(pos_cls[0][i] * rpn_stride + anchor_size // 2)\n","            }\n","            for i in range(len(pos_cls[0]))\n","        ]\n","    return anchors"],"metadata":{"id":"CVbfSSMnm189","executionInfo":{"status":"ok","timestamp":1731697351658,"user_tz":-120,"elapsed":19,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["\n","def get_anchor_gt(all_img_data, C, img_length_calc_function, mode='train', max_samples=100, batch_size=16, verbose=True):\n","    sample_count, batch_count = 0, 0\n","    batch_x_imgs, batch_y_rpn_cls, batch_y_rpn_regr, batch_img_data_augs, batch_num_pos_list, batch_anchors, debug_imgs = [], [], [], [], [], [], []\n","    num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)  # Calculate number of anchors locally\n","\n","    #print(all_img_data)\n","    TARGET_WIDTH, TARGET_HEIGHT = 600, 600\n","\n","\n","    while sample_count < max_samples:\n","        for img_data in all_img_data:\n","            #print(img_data)\n","            try:\n","                # Stop if max_samples reached\n","                if max_samples is not None and sample_count >= max_samples:\n","                    if batch_x_imgs:\n","                        yield (np.vstack(batch_x_imgs),\n","                               [np.vstack(batch_y_rpn_cls), np.vstack(batch_y_rpn_regr)],\n","                               batch_img_data_augs,\n","                               batch_num_pos_list,\n","                               batch_anchors,\n","                               debug_imgs)\n","                        print(debug_imgs)\n","                    return\n","\n","                img_path = img_data['filepath']\n","                x_img = cv2.imread(img_path)\n","                if x_img is None:\n","                    x_img = create_placeholder_image(TARGET_WIDTH, TARGET_HEIGHT)\n","                elif not isinstance(x_img, np.ndarray):\n","                    continue\n","\n","                # Resize and adjust image\n","                x_img, _ = resize_image_with_padding_or_cropping(x_img, TARGET_WIDTH, TARGET_HEIGHT)\n","# Update dimensions for this image\n","                img_data_instance = img_data.copy()\n","                img_data_instance['width'] = x_img.shape[1]\n","                img_data_instance['height'] = x_img.shape[0]\n","                img_data_instance['filepath'] = img_path  # Track each path individually\n","# Print the dimensions of the resized image to confirm\n","                #print(f\"Resized image dimensions: width={x_img.shape[1]}, height={x_img.shape[0]}\")\n","                if x_img.shape[:2] != (TARGET_HEIGHT, TARGET_WIDTH):\n","                    continue\n","# Update img_data to reflect the resized dimensions\n","                img_data['width'] = x_img.shape[1]\n","                img_data['height'] = x_img.shape[0]\n","\n","                if x_img.shape[:2] != (TARGET_HEIGHT, TARGET_WIDTH):\n","                    continue\n","                # Append processed image to debug_imgs\n","                debug_imgs.append(x_img.copy())\n","                #print(\"Priniting image copy\",x_img.copy())\n","                # print(\"i am here\")\n","\n","                # Calculate RPN ground truth\n","                y_rpn_cls, y_rpn_regr, num_pos, valid_anchors = calc_rpn(\n","                    C, img_data, TARGET_WIDTH, TARGET_HEIGHT, TARGET_WIDTH, TARGET_HEIGHT, img_length_calc_function\n","                )\n","                # Debug the ground truth shapes\n","                # In get_anchor_gt or wherever y_rpn_cls and y_rpn_regr are created\n","                # print(f\"Generated y_rpn_cls shape: {y_rpn_cls.shape}, Expected shape: (batch_size, {num_anchors}, height, width)\")\n","                # print(f\"Generated y_rpn_regr shape: {y_rpn_regr.shape}, Expected shape: (batch_size, {num_anchors * 4}, height, width)\")\n","                # print(\"num pos\",num_pos)\n","                # print(\"anchors in gt \",valid_anchors)\n","                # print(\"Shape of y_rpn_cls:\", y_rpn_cls.shape)\n","                # print(\"Shape of y_rpn_regr:\", y_rpn_regr.shape)\n","                # print(\"Length of valid_anchors:\", len(valid_anchors))\n","\n","                # print(f\"Generated y_rpn_cls shape: {y_rpn_cls.shape}, Expected shape: (batch_size, 18, height, width)\")\n","                # print(f\"Generated y_rpn_regr shape: {y_rpn_regr.shape}, Expected shape: (batch_size, 72, height, width)\")\n","# Check if no positive anchors generated\n","                # Handle case of no positive anchors\n","                if num_pos == 0:\n","                    print(f\"Warning: No positive anchors generated for image {img_data['filepath']}\")\n","                    # Initialize with zeros to maintain shape consistency\n","                    y_rpn_cls = np.zeros_like(y_rpn_cls)\n","                    y_rpn_regr = np.zeros_like(y_rpn_regr)\n","                # print(y_rpn_cls)\n","                # print(y_rpn_regr)\n","                # print(num_pos)\n","\n","                # Prepare image for batch stacking\n","                x_img = x_img.astype(np.float32)\n","                x_img = np.transpose(x_img, (2, 0, 1))\n","                x_img = np.expand_dims(x_img, axis=0)\n","\n","                # Append to batch lists\n","                batch_x_imgs.append(x_img)\n","                batch_y_rpn_cls.append(y_rpn_cls)\n","                batch_y_rpn_regr.append(y_rpn_regr)\n","                batch_img_data_augs.append(img_data_instance)\n","                batch_num_pos_list.append(num_pos)\n","                #print(batch_img_data_augs)\n","                if valid_anchors:\n","                    batch_anchors.append(np.array(valid_anchors))\n","                sample_count += 1\n","\n","                # Yield if batch is full\n","                if len(batch_x_imgs) == batch_size:\n","                    batch_count += 1\n","\n","\n","                    # Print the type of batch_anchors\n","                    # print(\"Type of batch_anchors:\", type(batch_anchors))\n","                    # print(len(batch_anchors))\n","                    # Check if batch_anchors is a list and contains arrays\n","                    # if isinstance(batch_anchors, list):\n","                    #     print(\"Total anchors:\", len(batch_anchors))\n","                    #     for i, anchor in enumerate(batch_anchors):\n","                    #         if hasattr(anchor, \"shape\"):\n","                    #             print(f\"Shape of anchor {i}:\", anchor.shape)\n","                    #         else:\n","                    #             print(f\"Anchor {i} is not an array and has type {type(anchor)}\")\n","                    # else:\n","                    #     # If batch_anchors is not a list, just print its shape if possible\n","                    #     print(\"Shape of batch_anchors:\", getattr(batch_anchors, \"shape\", \"No shape attribute\"))\n","\n","\n","\n","\n","\n","                    yield (np.vstack(batch_x_imgs),\n","                           [np.vstack(batch_y_rpn_cls), np.vstack(batch_y_rpn_regr)],\n","                           batch_img_data_augs,\n","                           batch_num_pos_list,\n","                           batch_anchors,\n","                          #  batch_X2_padded,\n","                           debug_imgs)\n","\n","\n","                    # Reset batch lists\n","                    batch_x_imgs, batch_y_rpn_cls, batch_y_rpn_regr, batch_img_data_augs, batch_num_pos_list, batch_anchors, debug_imgs = [], [], [], [], [], [], []\n","\n","            except Exception as e:\n","                print(f\"Error processing image {img_data['filepath']}: {e}\")\n","                continue"],"metadata":{"id":"vI1YUbjOMxGE","executionInfo":{"status":"ok","timestamp":1731697351659,"user_tz":-120,"elapsed":20,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZAAMEH4uqu9"},"source":["#### Define loss functions for all four outputs"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"CyLxnL4_uvmr","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":24,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["lambda_rpn_regr = 1.0\n","lambda_rpn_class = 1.0\n","\n","lambda_cls_regr = 1.0\n","lambda_cls_class = 1.0\n","\n","epsilon = 1e-4"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"tvGfH6m3yu0_","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":24,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def rpn_loss_regr(num_anchors):\n","    \"\"\"Loss function for rpn regression\n","    Args:\n","        num_anchors: number of anchors (9 in here)\n","    Returns:\n","        Smooth L1 loss function\n","                           0.5*x*x (if x_abs < 1)\n","                           x_abx - 0.5 (otherwise)\n","    \"\"\"\n","    def rpn_loss_regr_fixed_num(y_true, y_pred):\n","\n","        # x is the difference between true value and predicted vaue\n","        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n","\n","        # absolute value of x\n","        x_abs = K.abs(x)\n","\n","        # If x_abs <= 1.0, x_bool = 1\n","        x_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n","\n","        return lambda_rpn_regr * K.sum(\n","            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :, :4 * num_anchors])\n","\n","    return rpn_loss_regr_fixed_num\n","\n","\n","def rpn_loss_cls(num_anchors):\n","    \"\"\"Loss function for rpn classification\n","    Args:\n","        num_anchors: number of anchors (9 in here)\n","        y_true[:, :, :, :9]: [0,1,0,0,0,0,0,1,0] means only the second and the eighth box is valid which contains pos or neg anchor => isValid\n","        y_true[:, :, :, 9:]: [0,1,0,0,0,0,0,0,0] means the second box is pos and eighth box is negative\n","    Returns:\n","        lambda * sum((binary_crossentropy(isValid*y_pred,y_true))) / N\n","    \"\"\"\n","    def rpn_loss_cls_fixed_num(y_true, y_pred):\n","\n","            return lambda_rpn_class * K.sum(y_true[:, :, :, :num_anchors] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / K.sum(epsilon + y_true[:, :, :, :num_anchors])\n","\n","    return rpn_loss_cls_fixed_num\n","\n","\n","def class_loss_regr(num_classes):\n","    \"\"\"Loss function for rpn regression\n","    Args:\n","        num_anchors: number of anchors (9 in here)\n","    Returns:\n","        Smooth L1 loss function\n","                           0.5*x*x (if x_abs < 1)\n","                           x_abx - 0.5 (otherwise)\n","    \"\"\"\n","    def class_loss_regr_fixed_num(y_true, y_pred):\n","        x = y_true[:, :, 4*num_classes:] - y_pred\n","        x_abs = K.abs(x)\n","        x_bool = K.cast(K.less_equal(x_abs, 1.0), 'float32')\n","        return lambda_cls_regr * K.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(epsilon + y_true[:, :, :4*num_classes])\n","    return class_loss_regr_fixed_num\n","\n","\n","def class_loss_cls(y_true, y_pred):\n","    return lambda_cls_class * K.mean(categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n","\n","\n","def custom_accuracy(y_true, y_pred):\n","    \"\"\"\n","    Calculate accuracy for classification output.\n","    Assumes y_true and y_pred are in one-hot or probability format.\n","    \"\"\"\n","    # Convert probabilities to class predictions\n","    y_pred_class = K.argmax(y_pred, axis=-1)\n","    y_true_class = K.argmax(y_true, axis=-1)\n","\n","    # Compare predictions to true labels\n","    correct_preds = K.cast(K.equal(y_true_class, y_pred_class), tf.float32)\n","\n","    # Return mean accuracy over batch\n","    return K.mean(correct_preds)"]},{"cell_type":"code","source":["# import tensorflow as tf\n","# from tensorflow.keras import backend as K\n","\n","# def iou_loss(y_true, y_pred):\n","#     \"\"\"\n","#     Calculates IoU-based loss. This helps improve the mAP by encouraging\n","#     predicted boxes to have higher IoU with the ground truth boxes.\n","#     \"\"\"\n","\n","#     # Coordinates of the intersection box\n","#     xA = K.maximum(y_true[..., 0], y_pred[..., 0])\n","#     yA = K.maximum(y_true[..., 1], y_pred[..., 1])\n","#     xB = K.minimum(y_true[..., 2], y_pred[..., 2])\n","#     yB = K.minimum(y_true[..., 3], y_pred[..., 3])\n","\n","#     # Intersection area\n","#     interArea = K.maximum(0.0, xB - xA) * K.maximum(0.0, yB - yA)\n","\n","#     # Calculate areas of predicted and ground truth boxes\n","#     boxAArea = (y_true[..., 2] - y_true[..., 0]) * (y_true[..., 3] - y_true[..., 1])\n","#     boxBArea = (y_pred[..., 2] - y_pred[..., 0]) * (y_pred[..., 3] - y_pred[..., 1])\n","\n","#     # Union area\n","#     unionArea = boxAArea + boxBArea - interArea\n","\n","#     # IoU calculation\n","#     iou = interArea / K.maximum(unionArea, K.epsilon())\n","\n","#     # IoU loss (1 - IoU), to maximize IoU\n","#     iou_loss_value = 1 - iou\n","\n","#     return iou_loss_value\n","\n","# def focal_loss(gamma=2.0, alpha=0.25):\n","#     \"\"\"\n","#     Focal loss function for handling class imbalance.\n","#     \"\"\"\n","#     def focal_loss_fixed(y_true, y_pred):\n","#         # Clip predictions to prevent log(0)\n","#         y_pred = K.clip(y_pred, K.epsilon(), 1.0 - K.epsilon())\n","\n","#         # Compute focal loss components\n","#         cross_entropy_loss = -y_true * K.log(y_pred)\n","#         loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy_loss\n","\n","#         return K.sum(loss, axis=-1)\n","#     return focal_loss_fixed\n","\n","# def custom_mAP_loss(y_true_cls, y_pred_cls, y_true_reg, y_pred_reg):\n","#     \"\"\"\n","#     Custom mAP loss that combines classification loss, regression loss, and IoU loss.\n","#     - `y_true_cls`, `y_pred_cls`: classification ground truth and prediction.\n","#     - `y_true_reg`, `y_pred_reg`: regression ground truth and prediction.\n","#     \"\"\"\n","\n","#     # Classification loss (focal loss)\n","#     classification_loss = focal_loss()(y_true_cls, y_pred_cls)\n","\n","#     # Regression loss (smooth L1)\n","#     regression_loss = K.switch(\n","#         K.less(K.abs(y_true_reg - y_pred_reg), 1.0),\n","#         0.5 * K.square(y_true_reg - y_pred_reg),\n","#         K.abs(y_true_reg - y_pred_reg) - 0.5\n","#     )\n","\n","#     # IoU loss\n","#     iou_loss_value = iou_loss(y_true_reg, y_pred_reg)\n","\n","#     # Final combined loss\n","#     total_loss = K.mean(classification_loss + regression_loss + iou_loss_value)\n","\n","#     return total_loss"],"metadata":{"id":"nbsVOPtvSVak","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":23,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","execution_count":25,"metadata":{"id":"5cX0N4VDl4zS","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":23,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n","    # code used from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n","    # if there are no boxes, return an empty list\n","\n","    # Process explanation:\n","    #   Step 1: Sort the probs list\n","    #   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n","    #   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n","    #   Step 4: Repeat step 2 and step 3 until there is no item in the probs list\n","    if len(boxes) == 0:\n","        return []\n","\n","    # grab the coordinates of the bounding boxes\n","    x1 = boxes[:, 0]\n","    y1 = boxes[:, 1]\n","    x2 = boxes[:, 2]\n","    y2 = boxes[:, 3]\n","\n","    np.testing.assert_array_less(x1, x2)\n","    np.testing.assert_array_less(y1, y2)\n","\n","    # if the bounding boxes integers, convert them to floats --\n","    # this is important since we'll be doing a bunch of divisions\n","    if boxes.dtype.kind == \"i\":\n","        boxes = boxes.astype(\"float\")\n","\n","    # initialize the list of picked indexes\n","    pick = []\n","\n","    # calculate the areas\n","    area = (x2 - x1) * (y2 - y1)\n","\n","    # sort the bounding boxes\n","    idxs = np.argsort(probs)\n","\n","    # keep looping while some indexes still remain in the indexes\n","    # list\n","    while len(idxs) > 0:\n","        # grab the last index in the indexes list and add the\n","        # index value to the list of picked indexes\n","        last = len(idxs) - 1\n","        i = idxs[last]\n","        pick.append(i)\n","\n","        # find the intersection\n","\n","        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n","        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n","        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n","        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n","\n","        ww_int = np.maximum(0, xx2_int - xx1_int)\n","        hh_int = np.maximum(0, yy2_int - yy1_int)\n","\n","        area_int = ww_int * hh_int\n","\n","        # find the union\n","        area_union = area[i] + area[idxs[:last]] - area_int\n","\n","        # compute the ratio of overlap\n","        overlap = area_int/(area_union + 1e-6)\n","\n","        # delete all indexes from the index list that have\n","        idxs = np.delete(idxs, np.concatenate(([last],\n","            np.where(overlap > overlap_thresh)[0])))\n","\n","        if len(pick) >= max_boxes:\n","            break\n","\n","    # return only the bounding boxes that were picked using the integer data type\n","    boxes = boxes[pick].astype(\"int\")\n","    probs = probs[pick]\n","    return boxes, probs\n","\n","def apply_regr_np(X, T):\n","    \"\"\"Apply regression layer to all anchors in one feature map\n","\n","    Args:\n","        X: shape=(4, 18, 25) the current anchor type for all points in the feature map\n","        T: regression layer shape=(4, 18, 25)\n","\n","    Returns:\n","        X: regressed position and size for current anchor\n","    \"\"\"\n","    try:\n","        x = X[0, :, :]\n","        y = X[1, :, :]\n","        w = X[2, :, :]\n","        h = X[3, :, :]\n","\n","        tx = T[0, :, :]\n","        ty = T[1, :, :]\n","        tw = T[2, :, :]\n","        th = T[3, :, :]\n","\n","        cx = x + w/2.\n","        cy = y + h/2.\n","        cx1 = tx * w + cx\n","        cy1 = ty * h + cy\n","\n","        w1 = np.exp(tw.astype(np.float64)) * w\n","        h1 = np.exp(th.astype(np.float64)) * h\n","        x1 = cx1 - w1/2.\n","        y1 = cy1 - h1/2.\n","\n","        x1 = np.round(x1)\n","        y1 = np.round(y1)\n","        w1 = np.round(w1)\n","        h1 = np.round(h1)\n","        return np.stack([x1, y1, w1, h1])\n","    except Exception as e:\n","        print(e)\n","        return X\n","\n","def apply_regr(x, y, w, h, tx, ty, tw, th):\n","    # Apply regression to x, y, w and h\n","    try:\n","        cx = x + w/2.\n","        cy = y + h/2.\n","        cx1 = tx * w + cx\n","        cy1 = ty * h + cy\n","        w1 = math.exp(tw) * w\n","        h1 = math.exp(th) * h\n","        x1 = cx1 - w1/2.\n","        y1 = cy1 - h1/2.\n","        x1 = int(round(x1))\n","        y1 = int(round(y1))\n","        w1 = int(round(w1))\n","        h1 = int(round(h1))\n","\n","        return x1, y1, w1, h1\n","\n","    except ValueError:\n","        return x, y, w, h\n","    except OverflowError:\n","        return x, y, w, h\n","    except Exception as e:\n","        print(e)\n","        return x, y, w, h\n","\n","import numpy as np\n","import copy\n","\n","\n","def calc_iou(R_batch, img_data_batch, C, class_mapping, max_non_overlap_prints=5, fixed_max_rois=4):\n","    \"\"\"Converts from (x1, y1, x2, y2) to (x, y, w, h) format for batched input with fixed number of ROIs.\"\"\"\n","    batch_x_roi, batch_y_class_num, batch_y_class_regr_coords, batch_y_class_regr_label, batch_IoUs = [], [], [], [], []\n","\n","    for img_idx, (R, img_data) in enumerate(zip(R_batch, img_data_batch)):  # Process each image individually\n","        bboxes = img_data['bboxes']\n","        (width, height) = (img_data['width'], img_data['height'])\n","        resized_width, resized_height = width, height\n","        gta = np.zeros((len(bboxes), 4))\n","        non_overlap_count = 0  # Count for non-overlapping boxes\n","\n","        # Initialize lists to store results for each image\n","        x_roi, y_class_num, y_class_regr_coords, y_class_regr_label, IoUs = [], [], [], [], []\n","\n","        # Populate gta with resized ground-truth bounding box coordinates\n","        for bbox_num, bbox in enumerate(bboxes):\n","            gta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n","            gta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n","            gta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n","            gta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n","\n","        # Calculate IoUs for ROIs in the current image\n","        for ix in range(min(R.shape[0], fixed_max_rois)):\n","            x1, y1, x2, y2 = R[ix, :]\n","            x1 = int(round(x1 * C.rpn_stride))\n","            y1 = int(round(y1 * C.rpn_stride))\n","            x2 = int(round(x2 * C.rpn_stride))\n","            y2 = int(round(y2 * C.rpn_stride))\n","            best_iou, best_bbox = 0.0, -1\n","\n","            x1, y1 = max(0, min(x1, resized_width)), max(0, min(y1, resized_height))\n","            x2, y2 = max(0, min(x2, resized_width)), max(0, min(y2, resized_height))\n","\n","            # Iterate through all ground-truth bboxes to calculate the IoU\n","            for bbox_num in range(len(bboxes)):\n","                gt_x1, gt_y1, gt_x2, gt_y2 = gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]\n","                if x2 <= gt_x1 or x1 >= gt_x2 or y2 <= gt_y1 or y1 >= gt_y2:\n","                    non_overlap_count += 1\n","                    continue  # Skip this box since there's no overlap\n","\n","                # Calculate IoU\n","                curr_iou = iou([gt_x1, gt_y1, gt_x2, gt_y2], [x1, y1, x2, y2])\n","                if curr_iou > best_iou:\n","                    best_iou, best_bbox = curr_iou, bbox_num\n","\n","            if best_iou < C.classifier_min_overlap:\n","                continue\n","\n","            w, h = x2 - x1, y2 - y1\n","            x_roi.append([x1, y1, w, h])\n","            IoUs.append(best_iou)\n","\n","            # Assign class and regression targets\n","            if C.classifier_min_overlap <= best_iou < C.classifier_max_overlap:\n","                cls_name = 'bg'\n","            elif C.classifier_max_overlap <= best_iou:\n","                cls_name = bboxes[best_bbox]['class']\n","                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n","                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n","                cx, cy = x1 + w / 2.0, y1 + h / 2.0\n","                tx, ty = (cxg - cx) / float(w), (cyg - cy) / float(h)\n","                tw, th = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w)), np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n","            else:\n","                raise RuntimeError(\"Unexpected condition met for best_iou\")\n","\n","            class_num = class_mapping[cls_name]\n","            class_label = [0] * len(class_mapping)\n","            class_label[class_num] = 1\n","            y_class_num.append(copy.deepcopy(class_label))\n","            coords, labels = [0] * 4 * (len(class_mapping) - 1), [0] * 4 * (len(class_mapping) - 1)\n","\n","            if cls_name != 'bg':\n","                label_pos = 4 * class_num\n","                sx, sy, sw, sh = C.classifier_regr_std\n","                coords[label_pos:4 + label_pos] = [sx * tx, sy * ty, sw * tw, sh * th]\n","                labels[label_pos:4 + label_pos] = [1, 1, 1, 1]\n","            y_class_regr_coords.append(copy.deepcopy(coords))\n","            y_class_regr_label.append(copy.deepcopy(labels))\n","\n","        # Print ROIs and IoUs for the current image\n","\n","\n","\n","        # print(f\"Image {img_idx} - Non-zero ROIs and IoUs:\")\n","        # for roi, iou_value in zip(x_roi, IoUs):\n","        #     print(f\"ROI: {roi}, IoU: {iou_value}\")\n","\n","        # Ensure fixed number of ROIs by padding if necessary\n","        while len(x_roi) < fixed_max_rois:\n","            x_roi.append([0, 0, 0, 0])\n","            y_class_num.append([1] + [0] * (len(class_mapping) - 1))\n","            y_class_regr_coords.append([0] * 4 * (len(class_mapping) - 1))\n","            y_class_regr_label.append([0] * 4 * (len(class_mapping) - 1))\n","            IoUs.append(0.0)\n","\n","        # Only add if x_roi has valid entries\n","        if x_roi:\n","            batch_x_roi.append(np.expand_dims(np.array(x_roi), axis=0))\n","            batch_y_class_num.append(np.expand_dims(np.array(y_class_num), axis=0))\n","            batch_y_class_regr_coords.append(np.expand_dims(np.array(y_class_regr_coords), axis=0))\n","            batch_y_class_regr_label.append(np.expand_dims(np.array(y_class_regr_label), axis=0))\n","            batch_IoUs.append(IoUs)\n","\n","    # Check if no valid ROIs were found\n","    if not batch_x_roi:\n","        print(\"No valid ROIs found in batch, returning None.\")\n","        return None, None, None, None\n","\n","    # Stack batch elements if they exist\n","    batch_X2 = np.vstack(batch_x_roi)\n","    batch_Y1 = np.vstack(batch_y_class_num)\n","    batch_Y2 = np.vstack([\n","        np.concatenate([reg_label, reg_coords], axis=1)[:fixed_max_rois]\n","        for reg_label, reg_coords in zip(batch_y_class_regr_label, batch_y_class_regr_coords)\n","    ]).reshape(len(batch_x_roi), fixed_max_rois, -1)\n","\n","    return batch_X2, batch_Y1, batch_Y2, batch_IoUs\n","\n","\n","\n","\n","\n","\n","    # Stack all batch elements into arrays\n","    batch_X2 = np.vstack(batch_x_roi)\n","    batch_Y1 = np.vstack(batch_y_class_num)\n","    # batch_Y2 = np.vstack([np.concatenate([reg_label, reg_coords], axis=1) for reg_label, reg_coords in zip(batch_y_class_regr_label, batch_y_class_regr_coords)])\n","    # Ensure batch_Y2 has consistent structure and aligns with fixed_max_rois\n","    # Concatenate and limit to fixed_max_rois, then reshape to match batch size and fixed_max_rois\n","    batch_Y2 = np.vstack([\n","        np.concatenate([reg_label, reg_coords], axis=1)[:fixed_max_rois]  # Limit to fixed_max_rois\n","        for reg_label, reg_coords in zip(batch_y_class_regr_label, batch_y_class_regr_coords)\n","    ])\n","\n","    # Reshape batch_Y2 to (batch_size, fixed_max_rois, reg_output_dim) for alignment\n","    batch_Y2 = batch_Y2.reshape(batch_X2.shape[0], fixed_max_rois, -1)  # Adjusted shape: (batch_size, fixed_max_rois, reg_output_dim)\n","    return batch_X2, batch_Y1, batch_Y2, batch_IoUs\n","\n","\n","\n","\n","\n","\n","\n","def iou(boxA, boxB):\n","    # Determine the coordinates of the intersection rectangle\n","    xA = max(boxA[0], boxB[0])\n","    yA = max(boxA[1], boxB[1])\n","    xB = min(boxA[2], boxB[2])\n","    yB = min(boxA[3], boxB[3])\n","\n","    # Compute the area of intersection\n","    interArea = max(0, xB - xA) * max(0, yB - yA)\n","\n","    # Compute the area of both bounding boxes\n","    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n","    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n","\n","    # Compute the intersection over union\n","    iou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","    return iou"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"vT6X-fqJ1RSl","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":23,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["def rpn_to_roi(rpn_layer, regr_layer, C, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n","\t\"\"\"Convert rpn layer to roi bboxes\n","\n","\tArgs: (num_anchors = 9)\n","\t\trpn_layer: output layer for rpn classification\n","\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n","\t\t\tMight be (1, 18, 25, 18) if resized image is 400 width and 300\n","\t\tregr_layer: output layer for rpn regression\n","\t\t\tshape (1, feature_map.height, feature_map.width, num_anchors)\n","\t\t\tMight be (1, 18, 25, 72) if resized image is 400 width and 300\n","\t\tC: config\n","\t\tuse_regr: Wether to use bboxes regression in rpn\n","\t\tmax_boxes: max bboxes number for non-max-suppression (NMS)\n","\t\toverlap_thresh: If iou in NMS is larger than this threshold, drop the box\n","\n","\tReturns:\n","\t\tresult: boxes from non-max-suppression (shape=(300, 4))\n","\t\t\tboxes: coordinates for bboxes (on the feature map)\n","\t\"\"\"\n","\tregr_layer = regr_layer / C.std_scaling\n","\n","\tanchor_sizes = C.anchor_box_scales   # (3 in here)\n","\tanchor_ratios = C.anchor_box_ratios  # (3 in here)\n","\n","\tassert rpn_layer.shape[0] == 1\n","\n","\t(rows, cols) = rpn_layer.shape[1:3]\n","\n","\tcurr_layer = 0\n","\n","\t# A.shape = (4, feature_map.height, feature_map.width, num_anchors)\n","\t# Might be (4, 18, 25, 18) if resized image is 400 width and 300\n","\t# A is the coordinates for 9 anchors for every point in the feature map\n","\t# => all 18x25x9=4050 anchors cooridnates\n","\tA = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n","\n","\tfor anchor_size in anchor_sizes:\n","\t\tfor anchor_ratio in anchor_ratios:\n","\t\t\t# anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n","\t\t\t# anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n","\t\t\tanchor_x = (anchor_size * anchor_ratio[0])/C.rpn_stride\n","\t\t\tanchor_y = (anchor_size * anchor_ratio[1])/C.rpn_stride\n","\n","\t\t\t# curr_layer: 0~8 (9 anchors)\n","\t\t\t# the Kth anchor of all position in the feature map (9th in total)\n","\t\t\tregr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n","\t\t\tregr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n","\n","\t\t\t# Create 18x25 mesh grid\n","\t\t\t# For every point in x, there are all the y points and vice versa\n","\t\t\t# X.shape = (18, 25)\n","\t\t\t# Y.shape = (18, 25)\n","\t\t\tX, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n","\n","\t\t\t# Calculate anchor position and size for each feature map point\n","\t\t\tA[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n","\t\t\tA[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n","\t\t\tA[2, :, :, curr_layer] = anchor_x       # width of current anchor\n","\t\t\tA[3, :, :, curr_layer] = anchor_y       # height of current anchor\n","\n","\t\t\t# Apply regression to x, y, w and h if there is rpn regression layer\n","\t\t\tif use_regr:\n","\t\t\t\tA[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n","\n","\t\t\t# Avoid width and height exceeding 1\n","\t\t\tA[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n","\t\t\tA[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n","\n","\t\t\t# Convert (x, y , w, h) to (x1, y1, x2, y2)\n","\t\t\t# x1, y1 is top left coordinate\n","\t\t\t# x2, y2 is bottom right coordinate\n","\t\t\tA[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n","\t\t\tA[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n","\n","\t\t\t# Avoid bboxes drawn outside the feature map\n","\t\t\tA[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n","\t\t\tA[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n","\t\t\tA[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n","\t\t\tA[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n","\n","\t\t\tcurr_layer += 1\n","\n","\tall_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n","\tall_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n","\n","\tx1 = all_boxes[:, 0]\n","\ty1 = all_boxes[:, 1]\n","\tx2 = all_boxes[:, 2]\n","\ty2 = all_boxes[:, 3]\n","\n","\t# Find out the bboxes which is illegal and delete them from bboxes list\n","\tidxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n","\n","\tall_boxes = np.delete(all_boxes, idxs, 0)\n","\tall_probs = np.delete(all_probs, idxs, 0)\n","\n","\t# Apply non_max_suppression\n","\t# Only extract the bboxes. Don't need rpn probs in the later process\n","\tresult = non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n","\n","\treturn result"]},{"cell_type":"markdown","metadata":{"id":"Kk14GTaNmqoo"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oNsi6HtyJPSb"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TVmMqXE5x70U"},"source":["### Start training"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"C66bqGuOq7w6","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":23,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["base_path = '/content/drive/MyDrive/RCNN/Faster_RCNN_for_Open_Images_Dataset_Keras-master 2'\n","\n","train_path =  '/content/drive/MyDrive/datasets/coco/train/annotation.txt' # Training data (annotation file)\n","test_path =  '/content/drive/MyDrive/datasets/voc/annotation.txt' # Training data (annotation file)\n","\n","num_rois = 4 # Number of RoIs to process at once.\n","\n","# Augmentation flag\n","horizontal_flips = True # Augment with horizontal flips in training.\n","vertical_flips = True   # Augment with vertical flips in training.\n","rot_90 = True           # Augment with 90 degree rotations in training.\n","\n","output_weight_path = os.path.join(base_path, 'model/model_frcnn_vgg.hdf5')\n","\n","record_path = os.path.join(base_path, 'model/record.csv') # Record data (used to save the losses, classification accuracy and mean average precision)\n","\n","base_weight_path = os.path.join(base_path, 'model/vgg16_weights_tf_dim_ordering_tf_kernels.h5')\n","\n","config_output_filename = os.path.join(base_path, 'model_vgg_config.pickle')"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"J3oAmbbEutH0","executionInfo":{"status":"ok","timestamp":1731697351664,"user_tz":-120,"elapsed":22,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["# Create the config\n","C = Config()\n","\n","C.use_horizontal_flips = horizontal_flips\n","C.use_vertical_flips = vertical_flips\n","C.rot_90 = rot_90\n","\n","C.record_path = record_path\n","C.model_path = output_weight_path\n","C.num_rois = num_rois\n","\n","C.base_net_weights = base_weight_path"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":775116,"status":"ok","timestamp":1731698178633,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"},"user_tz":-120},"id":"yiEaAmb-x-so","outputId":"6ed4daa4-582c-4218-b155-5b4c17a73f23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing annotation files\n","idx=2943\n","Spend 12.91 mins to load the data\n"]}],"source":["#--------------------------------------------------------#\n","# This step will spend some time to load the data        #\n","#--------------------------------------------------------#\n","\n","# Set a limit for the number of images to load (e.g., 100)\n","limit = 1000\n","\n","# Call the get_data function with the limit\n","st = time.time()\n","train_imgs, classes_count, class_mapping = get_data(train_path, limit=limit)\n","\n","print()\n","print('Spend %0.2f mins to load the data' % ((time.time()-st)/60) )"]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4726,"status":"ok","timestamp":1731698183353,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"},"user_tz":-120},"id":"x-nuSdC56GsK","outputId":"4f448a68-8132-4ff7-ff91-eca2e6e5cd52"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training images per class:\n","{'aeroplane': 79,\n"," 'bear': 50,\n"," 'bg': 0,\n"," 'bicycle': 21,\n"," 'bird': 241,\n"," 'boat': 96,\n"," 'bus': 18,\n"," 'car': 168,\n"," 'cat': 93,\n"," 'chair': 130,\n"," 'couch': 41,\n"," 'cow': 231,\n"," 'diningtable': 86,\n"," 'dog': 65,\n"," 'elephant': 148,\n"," 'giraffe': 171,\n"," 'horse': 76,\n"," 'motorbike': 32,\n"," 'person': 350,\n"," 'pottedplant': 102,\n"," 'sheep': 196,\n"," 'train': 130,\n"," 'truck': 112,\n"," 'tvmonitor': 97,\n"," 'zebra': 210}\n","Num classes (including bg) = 25\n","{'cow': 0, 'train': 1, 'dog': 2, 'sheep': 3, 'chair': 4, 'diningtable': 5, 'pottedplant': 6, 'truck': 7, 'person': 8, 'aeroplane': 9, 'motorbike': 10, 'zebra': 11, 'cat': 12, 'giraffe': 13, 'bear': 14, 'bicycle': 15, 'boat': 16, 'elephant': 17, 'car': 18, 'horse': 19, 'couch': 20, 'tvmonitor': 21, 'bird': 22, 'bus': 23, 'bg': 24}\n","Config has been written to /content/drive/MyDrive/RCNN/Faster_RCNN_for_Open_Images_Dataset_Keras-master 2/model_vgg_config.pickle, and can be loaded when testing to ensure correct results\n"]}],"source":["if 'bg' not in classes_count:\n","\tclasses_count['bg'] = 0\n","\tclass_mapping['bg'] = len(class_mapping)\n","# e.g.\n","#    classes_count: {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745, 'bg': 0}\n","#    class_mapping: {'Person': 0, 'Car': 1, 'Mobile phone': 2, 'bg': 3}\n","C.class_mapping = class_mapping\n","\n","print('Training images per class:')\n","pprint.pprint(classes_count)\n","print('Num classes (including bg) = {}'.format(len(classes_count)))\n","print(class_mapping)\n","\n","# Save the configuration\n","with open(config_output_filename, 'wb') as config_f:\n","\tpickle.dump(C,config_f)\n","\tprint('Config has been written to {}, and can be loaded when testing to ensure correct results'.format(config_output_filename))\n"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1731698183353,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"},"user_tz":-120},"id":"LFlq36Sx4F4O","outputId":"db1c3148-b6c3-42b2-9d92-a6406b06a292"},"outputs":[{"output_type":"stream","name":"stdout","text":["Num train samples (images) 1000\n"]}],"source":["# # Shuffle the images with seed\n","# random.seed(1)\n","# random.shuffle(train_imgs)\n","\n","print('Num train samples (images) {}'.format(len(train_imgs)))"]},{"cell_type":"markdown","metadata":{"id":"y_yM5jkKqM1G"},"source":["#### Explore 'data_gen_train'\n","\n","data_gen_train is an **generator**, so we get the data by calling **next(data_gen_train)**"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","# Function to load dataset information (this assumes your dataset loading function is 'get_data')\n","def calculate_average_image_size(image_data):\n","    total_width = 0\n","    total_height = 0\n","    num_images = len(image_data)\n","\n","    for img_info in image_data:\n","        # Load the image using the filepath\n","        img = cv2.imread(img_info['filepath'])\n","        if img is None:\n","            print(f\"Warning: Could not load image at {img_info['filepath']}. Skipping.\")\n","            num_images -= 1\n","            continue\n","\n","        # Get the width and height of the image\n","        height, width = img.shape[:2]\n","\n","        total_width += width\n","        total_height += height\n","\n","    # Calculate the average width and height\n","    avg_width = total_width / num_images\n","    avg_height = total_height / num_images\n","\n","    return avg_width, avg_height\n","\n","\n","# Calculate and print the average image size\n","avg_width, avg_height = calculate_average_image_size(train_imgs)\n","print(f\"Average image width: {avg_width}\")\n","print(f\"Average image height: {avg_height}\")"],"metadata":{"id":"HQT_drS21D62","executionInfo":{"status":"ok","timestamp":1731698188026,"user_tz":-120,"elapsed":4695,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cedb916b-b083-4da0-b95c-9295f1ce2678"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Average image width: 591.086\n","Average image height: 471.141\n"]}]},{"cell_type":"code","source":["# import cv2\n","# import matplotlib.pyplot as plt\n","# import numpy as np\n","\n","# # Load dataset information (metadata, bounding boxes, etc.)\n","# train_imgs, classes_count, class_mapping = get_data(train_path, limit=4000)  # Load only one image for simplicity\n","# data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train', batch_size=2, max_samples=4000)  # Set batch_size=1\n","\n","# # Fetch and display a single sample\n","# try:\n","#     # Get the next item from the generator (one image)\n","#     X, Y, img_data, num_pos_list, anchors, debug_img = next(data_gen_train)\n","#     print(f\"Shape of y_rpn_cls: {Y[0].shape}\")\n","\n","#     for idx, (img, image_data) in enumerate(zip(debug_img, img_data)):\n","#     #for idx, (img, image_data, y_rpn_cls, y_rpn_regr) in enumerate(zip(debug_imgs, img_data, Y[0], Y[1])):\n","#         print(f\"\\nProcessing Image {idx + 1}\")\n","#         y_rpn_cls, y_rpn_regr = Y[0][idx], Y[1][idx]\n","#         print(\"Shape of y_rpn_cls:\", y_rpn_cls.shape)\n","#         print(\"Shape of y_rpn_regr:\", y_rpn_regr.shape)\n","#     # Continue processing with img, image_data, y_rpn_cls, and y_rpn_regr\n","\n","#         # Check if image_data is a dictionary as expected\n","#         if isinstance(image_data, dict) and 'bboxes' in image_data:\n","#             # Use original bounding boxes for the current image\n","#             original_bboxes = image_data['bboxes']\n","#         else:\n","#             print(f\"Warning: Image data at index {idx} is not in the expected format.\")\n","#             continue  # Skip to the next image if the format is unexpected\n","\n","#                 # Use the first channel as an example (this may vary based on your anchor setup)\n","#         cls = y_rpn_cls[0]  # Extracting the first channel for positive anchor classification\n","#         pos_cls = np.where(cls == 1)\n","\n","#         print(f\"Shape of cls for Image {idx + 1}: {cls.shape}\")\n","#         print(f\"Shape of pos_cls for Image {idx + 1}: {pos_cls[0].shape}\")\n","\n","\n","\n","#         # Display the image with bounding boxes and anchors overlaid\n","#         # draw_boxes_and_anchors(img, original_bboxes, anchors)\n","\n","# except StopIteration:\n","#     print(\"Reached the end of the dataset.\")"],"metadata":{"id":"I9L2dRjPuBGA","executionInfo":{"status":"ok","timestamp":1731698188026,"user_tz":-120,"elapsed":4,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i4XSyIoubCMY"},"source":["#### Build the model"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"jODipXFDnDJ0","executionInfo":{"status":"ok","timestamp":1731698189179,"user_tz":-120,"elapsed":1156,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["input_shape_img = (None, None, 3)\n","\n","img_input = Input(shape=input_shape_img)\n","roi_input = Input(shape=(None, 4))\n","\n","# define the base network (VGG here, can be Resnet50, Inception, etc)\n","shared_layers = nn_base(img_input, trainable=True)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"udTeQMVhfSzw","executionInfo":{"status":"ok","timestamp":1731698189179,"user_tz":-120,"elapsed":12,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0f5c167-bff5-4422-c8a1-7a12f46184ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Anchor box scales: [64, 128, 256]\n","Anchor box ratios: [[1, 1], [1, 2], [2, 1], [1, 3], [3, 1], [2, 3]]\n","Calculated number of anchors: 18\n","x_class shape (classification output): (None, None, None, 36)\n","x_regr shape (regression output): (None, None, None, 72)\n","25\n","Using 25 classes for classifier and 96 outputs for regression\n","This is the first time of your training\n","loading weights from /content/drive/MyDrive/RCNN/Faster_RCNN_for_Open_Images_Dataset_Keras-master 2/model/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","Could not load pretrained model weights. Weights can be found in the keras application folder             https://github.com/fchollet/keras/tree/master/keras/applications\n"]}],"source":["from tensorflow.keras.layers import Layer\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.layers import Conv2D, TimeDistributed, Dense, Flatten\n","\n","# Print anchor configurations\n","print(f\"Anchor box scales: {C.anchor_box_scales}\")\n","print(f\"Anchor box ratios: {C.anchor_box_ratios}\")\n","num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n","print(f\"Calculated number of anchors: {num_anchors}\")\n","rpn = rpn_layer(shared_layers, num_anchors)\n","print(len(classes_count))\n","classifier = classifier_layer(shared_layers, roi_input, C.num_rois, nb_classes=len(classes_count))\n","\n","model_rpn = Model(img_input, rpn[:2])\n","model_classifier = Model([img_input, roi_input], classifier)\n","\n","# this is a model that holds both the RPN and the classifier, used to load/save weights for the models\n","model_all = Model([img_input, roi_input], rpn[:2] + classifier)\n","\n","# Because the google colab can only run the session several hours one time (then you need to connect again),\n","# we need to save the model and load the model to continue training\n","if not os.path.isfile(C.model_path):\n","    #If this is the begin of the training, load the pre-traind base network such as vgg-16\n","    try:\n","        print('This is the first time of your training')\n","        print('loading weights from {}'.format(C.base_net_weights))\n","        model_rpn.load_weights(C.base_net_weights, by_name=True)\n","        model_classifier.load_weights(C.base_net_weights, by_name=True)\n","    except:\n","        print('Could not load pretrained model weights. Weights can be found in the keras application folder \\\n","            https://github.com/fchollet/keras/tree/master/keras/applications')\n","\n","    # Create the record.csv file to record losses, acc and mAP\n","    record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n","else:\n","    # If this is a continued training, load the trained model from before\n","    print('Continue training based on previous trained model')\n","    print('Loading weights from {}'.format(C.model_path))\n","    model_rpn.load_weights(C.model_path, by_name=True)\n","    model_classifier.load_weights(C.model_path, by_name=True)\n","\n","    # Load the records\n","    record_df = pd.read_csv(record_path)\n","\n","    r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n","    r_class_acc = record_df['class_acc']\n","    r_loss_rpn_cls = record_df['loss_rpn_cls']\n","    r_loss_rpn_regr = record_df['loss_rpn_regr']\n","    r_loss_class_cls = record_df['loss_class_cls']\n","    r_loss_class_regr = record_df['loss_class_regr']\n","    r_curr_loss = record_df['curr_loss']\n","    r_elapsed_time = record_df['elapsed_time']\n","    r_mAP = record_df['mAP']\n","\n","    print('Already train %dK batches'% (len(record_df)))"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"-ULrg0V1soIR","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":11,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["from keras.optimizers import Adam, SGD\n","\n","# Define learning rates for different components\n","learning_rate_rpn = 1e-5\n","learning_rate_classifier = 1e-5\n","learning_rate_all = 1e-4  # For the 'model_all' compilation\n","\n","# Define optimizers with specified learning rates\n","optimizer_rpn = Adam(learning_rate=learning_rate_rpn)\n","optimizer_classifier = Adam(learning_rate=learning_rate_classifier)\n","optimizer_all = SGD(learning_rate=learning_rate_all)\n","\n","# Compile the RPN model with its specific loss functions\n","model_rpn.compile(\n","    optimizer=optimizer_rpn,\n","    loss={\n","        'rpn_out_class': rpn_loss_cls(num_anchors),\n","        'rpn_out_regress': rpn_loss_regr(num_anchors)\n","    }\n",")\n","\n","model_classifier.compile(\n","    optimizer=optimizer_classifier,\n","    loss={\n","        'dense_class_25': class_loss_cls,\n","        'dense_regress_25': class_loss_regr(len(classes_count) - 1)\n","    },\n","    metrics={\n","        'dense_class_25': custom_accuracy # Calculate accuracy for the classification output\n","    }\n",")\n","\n","# model_classifier.compile(\n","#     optimizer=optimizer_classifier,\n","#     loss=lambda y_true, y_pred: custom_mAP_loss(\n","#         y_true_cls=y_true[0],   # Classification labels\n","#         y_pred_cls=y_pred[0],   # Classification predictions\n","#         y_true_reg=y_true[1],   # Regression labels\n","#         y_pred_reg=y_pred[1]    # Regression predictions\n","#     ),\n","#     metrics={\n","#         'dense_class_25': custom_accuracy  # Accuracy for the classification output\n","#     }\n","# )\n","\n","# Compile the full model (model_all) with a simpler loss function, typically used for overall model adjustments\n","model_all.compile(optimizer=optimizer_all, loss='mae')"]},{"cell_type":"code","source":["y_true_dummy = np.random.randint(0, 2, size=(4, 4, 25))\n","y_pred_dummy = np.random.rand(4, 4, 25)\n","print(\"Custom accuracy test output:\", custom_accuracy(y_true_dummy, y_pred_dummy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_SldW2s6pCl","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":11,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"outputId":"6d1a5c58-a167-4a82-8aae-26a685c5a520"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Custom accuracy test output: tf.Tensor(0.0625, shape=(), dtype=float32)\n"]}]},{"cell_type":"code","execution_count":39,"metadata":{"id":"Qz2BYzL6sqfu","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":10,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d71e41f1-e5e3-46c1-cfbc-fc6b05f52c4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset contains 1000 images.\n","Starting training for 10 epochs, with 63 iterations per epoch.\n"]}],"source":["# Assuming `train_imgs` is your dataset, and it's a list or array of image data\n","total_images = len(train_imgs)  # Dynamically get the total number of images\n","batch_size = 16  # Set batch size (can adjust based on memory)\n","epoch_length = np.ceil(total_images / batch_size).astype(int)  # Calculate epoch length dynamically\n","num_epochs = 10  # Set number of epochs (can adjust as needed)\n","\n","iter_num = 0\n","losses = np.zeros((epoch_length, 5))  # For tracking losses and accuracy\n","\n","# RPN accuracy monitoring\n","rpn_accuracy_rpn_monitor = []\n","rpn_accuracy_for_epoch = []\n","\n","# Adjust best_loss based on previous runs or start fresh\n","if record_df.empty:\n","    best_loss = np.Inf  # Start with infinity for best loss if no prior record\n","else:\n","    best_loss = record_df['curr_loss'].min()  # Fetch the lowest recorded loss\n","\n","print(f\"Dataset contains {total_images} images.\")\n","print(f\"Starting training for {num_epochs} epochs, with {epoch_length} iterations per epoch.\")"]},{"cell_type":"code","source":["print(model_classifier.metrics_names)\n","print(model_rpn.metrics_names)\n","print(\"model_rpn outputs:\", {output.name: output.shape for output in model_rpn.outputs})\n","print(\"model_classifier outputs:\", {output.name: output.shape for output in model_classifier.outputs})\n","# print([layer.name for layer in model_rpn.output])\n","# print([layer.name for layer in model_classifier.output])\n","\n","# model_rpn.summary()\n","# model_classifier.summary()\n","# model_rpn.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c93-Kxn2pDbJ","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":9,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"outputId":"49508d8f-81fc-4c7c-941e-b9e58e78a6e5"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["['loss', 'compile_metrics']\n","['loss']\n","model_rpn outputs: {'keras_tensor_20': (None, None, None, 36), 'keras_tensor_21': (None, None, None, 72)}\n","model_classifier outputs: {'keras_tensor_28': (None, 4, 25), 'keras_tensor_29': (None, 4, 96)}\n"]}]},{"cell_type":"code","source":["# # Assuming `get_anchor_gt` is already defined and available\n","\n","# # Instantiate the generator\n","# data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train', batch_size=16, max_samples=32)\n","\n","# # Initialize a variable to keep track of the maximum number of anchors\n","# max_num_anchors = 0\n","\n","# # Iterate through the generator to process each image individually\n","# for batch_idx, batch in enumerate(data_gen_train):\n","#     # Unpack batch to get the components\n","#     images, [y_rpn_cls, y_rpn_regr], img_data_augs, num_pos_list, anchors, debug_imgs = batch\n","\n","#     # Print details of anchors\n","#     print(\"Type of anchors:\", type(anchors))\n","#     print(\"Shape of anchors:\", np.array(anchors).shape)  # Check shape\n","#     print(\"Sample anchors:\", anchors[:5])  # Sample few anchors\n","\n","#     # Calculate the number of anchors for the current image\n","#     num_anchors = len(anchors)\n","#     print(f\"Image {batch_idx + 1}: Number of anchors = {num_anchors}\")\n","\n","#     # Print shapes of y_rpn_cls and y_rpn_regr\n","#     print(\"Shape of y_rpn_cls:\", y_rpn_cls.shape)\n","#     print(\"Shape of y_rpn_regr:\", y_rpn_regr.shape)\n","\n","#     # Check the positive anchors list\n","#     print(\"num_pos_list:\", num_pos_list)\n","#     print(\"Sum of positive anchors for this batch:\", sum(num_pos_list))\n","\n","#     # Print augmentation details and bounding boxes\n","#     print(\"Image augmentations and bounding boxes:\", img_data_augs)\n","#     print(\"Bounding box coordinates:\", [(bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']) for bbox in img_data_augs[0]['bboxes']])\n","\n","#     # Update max_num_anchors if this image has more anchors than the current maximum\n","#     max_num_anchors = max(max_num_anchors, num_anchors)\n","\n","#     # Break if maximum samples reached\n","#     if batch_idx + 1 >= 32:  # Adjust according to your max_samples limit\n","#         break\n","\n","# # Print the maximum number of anchors encountered\n","# print(f\"Maximum number of anchors across all images: {max_num_anchors}\")"],"metadata":{"id":"OX5ClcnJweot","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":8,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["import random\n","import sys\n","import cv2\n","\n","def get_dataset(input_path, limit=None, target_per_class=50, margin=0.5):\n","    \"\"\"\n","    Parse the data from an annotation file and balance the class distribution.\n","\n","    Args:\n","        input_path: Annotation file path.\n","        limit: Optional limit to load only a certain number of images.\n","        target_per_class: Target minimum number of images per class.\n","        margin: Allowable margin to include more than the target for rare classes.\n","\n","    Returns:\n","        all_data: list of dictionaries with keys: 'filepath', 'width', 'height', 'bboxes'.\n","        classes_count: dict with class name as key and count as value.\n","        class_mapping: dict with class name as key and index as value.\n","    \"\"\"\n","    found_bg = False\n","    all_imgs = {}\n","    classes_count = {}\n","    class_mapping = {}\n","    i = 1\n","\n","    # Parse annotation file\n","    with open(input_path, 'r') as f:\n","        print('Parsing annotation files')\n","\n","        for line in f:\n","            # Stop if a limit is set and reached\n","            if limit and len(all_imgs) >= limit:\n","                break\n","\n","            sys.stdout.write('\\r' + 'idx=' + str(i))\n","            i += 1\n","            line_split = line.strip().split(',')\n","            (filename, x1, y1, x2, y2, class_name) = line_split\n","\n","            # Track class counts\n","            if class_name not in classes_count:\n","                classes_count[class_name] = 1\n","            else:\n","                classes_count[class_name] += 1\n","\n","            # Update class mapping\n","            if class_name not in class_mapping:\n","                if class_name == 'bg' and not found_bg:\n","                    print('Found class name \"bg\". Will treat as background.')\n","                    found_bg = True\n","                class_mapping[class_name] = len(class_mapping)\n","\n","            # Add image details\n","            if filename not in all_imgs:\n","                all_imgs[filename] = {}\n","                img = cv2.imread(filename)\n","                if img is None:\n","                    print(f\"Warning: {filename} could not be loaded.\")\n","                    continue\n","\n","                (rows, cols) = img.shape[:2]\n","                all_imgs[filename]['filepath'] = filename\n","                all_imgs[filename]['width'] = cols\n","                all_imgs[filename]['height'] = rows\n","                all_imgs[filename]['bboxes'] = []\n","\n","            # Append the bounding box for this image\n","            all_imgs[filename]['bboxes'].append({\n","                'class': class_name,\n","                'x1': int(x1),\n","                'x2': int(x2),\n","                'y1': int(y1),\n","                'y2': int(y2)\n","            })\n","\n","    # Filter images to balance the class distribution\n","    target_instances = max(target_per_class, min(classes_count.values()) * (1 + margin))\n","    balanced_data = []\n","    per_class_data = {cls: [] for cls in classes_count.keys()}\n","\n","    for img_info in all_imgs.values():\n","        for bbox in img_info['bboxes']:\n","            per_class_data[bbox['class']].append(img_info)\n","\n","    for cls, data in per_class_data.items():\n","        if len(data) > target_instances:\n","            sampled_data = random.sample(data, target_instances)\n","            balanced_data.extend(sampled_data)\n","            print(f\"Reduced {cls} from {len(data)} to {target_instances} samples\")\n","        else:\n","            balanced_data.extend(data)\n","            print(f\"Retained all {len(data)} instances of {cls}\")\n","\n","    # Ensure 'bg' class is included in classes_count and class_mapping\n","    if 'bg' not in classes_count:\n","        classes_count['bg'] = 0\n","    if 'bg' not in class_mapping:\n","        class_mapping['bg'] = len(class_mapping)\n","\n","    # Ensure background class is last in mapping\n","    if found_bg:\n","        if class_mapping['bg'] != len(class_mapping) - 1:\n","            key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping) - 1][0]\n","            val_to_switch = class_mapping['bg']\n","            class_mapping['bg'] = len(class_mapping) - 1\n","            class_mapping[key_to_switch] = val_to_switch\n","\n","    print(f\"Class counts after balancing: {dict(Counter([bbox['class'] for img in balanced_data for bbox in img['bboxes']]))}\")\n","    return balanced_data, classes_count, class_mapping"],"metadata":{"id":"dKIJCxD3-Iom","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":8,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["def compute_map(true_boxes, predicted_boxes, iou_threshold=0.4):\n","    tp = 0\n","    fp = 0\n","    iou_scores = []\n","\n","    # Ensure each box has accessible coordinates\n","    def get_box_coords(box):\n","        # Modify based on box structure if it's a dict\n","        if isinstance(box, dict):\n","            return [box['x1'], box['y1'], box['x2'], box['y2']]\n","        return box  # Assume list, tuple, or np.array\n","\n","    # Calculate IoU for each pair of true and predicted boxes\n","    for true_box in true_boxes:\n","        true_box = get_box_coords(true_box)\n","        max_iou = 0\n","        for pred_box in predicted_boxes:\n","            pred_box = get_box_coords(pred_box)\n","\n","            # array = np.array(pred_box)\n","            # print(array.shape)\n","            # array = np.array(true_box)\n","            # print(array.shape)\n","            # current_iou = iou(true_box, pred_box)\n","            # print(current_iou)\n","            max_iou = max(max_iou, current_iou)\n","        iou_scores.append(max_iou)\n","\n","\n","        # Determine if it's a TP or FP based on the IoU threshold\n","        if max_iou >= iou_threshold:\n","            tp += 1\n","        else:\n","            fp += 1\n","\n","    # Calculate precision and recall\n","    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","    recall = tp / len(true_boxes) if len(true_boxes) > 0 else 0\n","\n","    # Calculate the mean Average Precision (mAP)\n","    mAP = np.mean(precision) if isinstance(precision, np.ndarray) else precision\n","\n","    return mAP"],"metadata":{"id":"GSrKLjiVfZii","executionInfo":{"status":"ok","timestamp":1731698189180,"user_tz":-120,"elapsed":7,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["import traceback\n","from keras.utils import Progbar\n","import numpy as np\n","import time\n","import os\n","import pandas as pd\n","from keras.losses import categorical_crossentropy\n","# tf.config.run_functions_eagerly(True)\n","# Early stopping parameters\n","patience = 5\n","no_improvement_count = 0\n","best_loss = np.Inf\n","improvement_threshold = 1e-4  # Define a threshold for minimum improvement\n","\n","# Load dataset information (metadata, bounding boxes, etc.)\n","train_imgs, classes_count, class_mapping = get_data(train_path, limit=1000)  # Load only one image for simplicity\n","\n","# Set batch size and calculate epoch length\n","batch_size = 4\n","epoch_length = len(train_imgs) // batch_size\n","print(f\"Epoch length: {epoch_length}\")\n","# Ensure model path is correctly formatted and directory exists\n","C.model_path = C.model_path.replace(\".hdf5\", \".weights.h5\")\n","model_dir = os.path.dirname(C.model_path)\n","if not os.path.exists(model_dir):\n","    os.makedirs(model_dir)\n","\n","# Compile models to initialize weights randomly\n","# model_rpn.compile(optimizer='adam', loss='mse')  # Example loss; adjust to your model's needs\n","# model_classifier.compile(optimizer='adam', loss='mse')\n","model_rpn.compile(optimizer='adam',\n","                 loss={'rpn_out_class': 'binary_crossentropy',\n","                       'rpn_out_regress': 'mse'})\n","# Check if weights are initialized\n","initial_weights_rpn = model_rpn.get_weights()\n","initial_weights_classifier = model_classifier.get_weights()\n","\n","# Check if weights are non-empty\n","print(\"Initial weights check:\")\n","if initial_weights_rpn:\n","    print(\"RPN Model weights are initialized.\")\n","else:\n","    print(\"RPN Model weights are not initialized!\")\n","\n","if initial_weights_classifier:\n","    print(\"Classifier Model weights are initialized.\")\n","else:\n","    print(\"Classifier Model weights are not initialized!\")\n","\n","# Initialize a DataFrame to track metrics\n","record_df = pd.DataFrame(columns=[\n","    'mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls',\n","    'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr',\n","    'curr_loss', 'elapsed_time', 'mAP'\n","])\n","\n","losses = np.zeros((epoch_length, 5))\n","start_time = time.time()\n","batch_losses_log = []  # To log each batch loss\n","\n","# Training loop\n","for epoch_num in range(num_epochs):\n","    progbar = Progbar(epoch_length)\n","    print(f'\\nEpoch {epoch_num + 1}/{num_epochs}')\n","\n","    data_gen_train = get_anchor_gt(train_imgs, C, get_img_output_length, mode='train', batch_size=4, max_samples=1000)  # Set batch_size=1\n","\n","\n","    iter_num = 0\n","    epoch_loss = 0\n","    true_boxes_list = []  # Store true boxes\n","    predicted_boxes_list = []  # Store predicted boxes\n","\n","    for X_batch, Y_batch, img_data_batch, num_pos_batch, anchors, debug_imgs_batch  in data_gen_train:\n","        try:\n","\n","            # Step 1: Detailed Batch Data Print\n","            # print(f\"\\n--- Epoch {epoch_num + 1}, Batch {iter_num + 1} ---\")\n","            # print(\"Checking if batch is non-empty and correctly shaped...\")\n","            # print(\"Y_batch[0] (classification) shape:\", Y_batch[0].shape)  # Expected: (batch_size, h, w, num_anchors)\n","            # print(\"Y_batch[1] (regression) shape:\", Y_batch[1].shape)  # Expected: (batch_size, h, w, num_anchors * 4)\n","            # print(\"Number of anchors:\", num_pos_batch)\n","\n","\n","\n","\n","\n","            # print(f\"X_batch shape: {X_batch.shape}\")\n","            X_batch = np.transpose(X_batch, (0, 2, 3, 1))  # Transpose from (batch_size, channels, height, width) to (batch_size, height, width, channels)\n","            # print(\"Shape of X_batch after transpose:\", X_batch.shape)\n","            if X_batch.size == 0 or Y_batch[0].size == 0 or Y_batch[1].size == 0:\n","                print(\"Empty batch detected. Skipping iteration.\")\n","                continue\n","            # Step 2: Train the RPN and Verify RPN Output\n","            loss_rpn = model_rpn.train_on_batch(X_batch, Y_batch)\n","\n","\n","            # Print the value of loss_rpn for debugging\n","            # print(f\"Debug: loss_rpn = {loss_rpn}, type = {type(loss_rpn)}\")\n","            # print(\"losses.shape:\", losses.shape)\n","            # print(\"iter_num:\", iter_num)\n","\n","\n","            # Adjust loss handling based on the format of loss_rpn\n","            if loss_rpn is None:\n","                print(\"Warning: loss_rpn is None. Setting default values for losses.\")\n","                losses[iter_num, 0] = 0\n","                losses[iter_num, 1] = 0\n","            elif isinstance(loss_rpn, np.ndarray) and loss_rpn.ndim == 0:\n","                # Handle numpy scalar array by converting it to a Python float\n","                losses[iter_num, 0] = float(loss_rpn)  # Convert to float for safe handling\n","                losses[iter_num, 1] = 0\n","            elif np.isscalar(loss_rpn):\n","                # Directly handle regular Python scalars\n","                losses[iter_num, 0] = float(loss_rpn)\n","                losses[iter_num, 1] = 0\n","            elif isinstance(loss_rpn, (list, tuple)):\n","                # Multiple outputs (list or tuple)\n","                losses[iter_num, 0] = loss_rpn[0] if len(loss_rpn) > 0 else 0\n","                losses[iter_num, 1] = loss_rpn[1] if len(loss_rpn) > 1 else 0\n","            else:\n","                raise ValueError(\"Unexpected format for loss_rpn:\", loss_rpn)\n","            # Predict RPN outputs and check shapes and content\n","            P_rpn = model_rpn.predict_on_batch(X_batch)\n","\n","\n","\n","            # Generate ROIs for the entire batch at once\n","            R_batch = [rpn_to_roi(\n","                          np.expand_dims(P_rpn[0][img_idx], axis=0),\n","                          np.expand_dims(P_rpn[1][img_idx], axis=0),\n","                          C, use_regr=True, overlap_thresh=0.5, max_boxes=300,\n","                          dim_ordering=\"channels_last\"\n","                      )\n","                      for img_idx in range(batch_size)]\n","\n","\n","            # print(\"R batch\",R_batch)\n","\n","\n","\n","            # Call calc_iou directly with batch input\n","            batch_X2, batch_Y1, batch_Y2, IoUs_batch = calc_iou(R_batch, img_data_batch, C, class_mapping)\n","\n","            # Confirm shapes are as expected before train_on_batch\n","\n","\n","            # print(\"Batch dimensions before train_on_batch:\")\n","            # print(f\"batch_X2 shape: {batch_X2.shape}\")\n","            # print(f\"batch_Y1 shape: {batch_Y1.shape}\")\n","            # print(f\"batch_Y2 shape: {batch_Y2.shape}\")\n","            # print(\"batchx\",X_batch.shape)\n","            # print(\"Shape of batch_Y1 (classification labels):\", batch_Y1.shape)\n","            # print(\"Shape of batch_Y2 (regression labels):\", batch_Y2.shape)\n","\n","                        # Add check to handle case when there are no ROIs\n","            if batch_X2 is None:\n","                print(\"Skipping batch due to no valid ROIs.\")\n","                continue  # Skip processing this batch and move to the next one\n","\n","            batch_size = batch_X2.shape[0]  # Confirm batch size from `X_batch`\n","            num_rois = batch_X2.shape[1]\n","\n","            # Flatten ROIs and indices to check\n","            rois_flat = batch_X2.reshape(-1, 4)  # Flatten to (batch_size * num_rois, 4)\n","            box_indices = np.repeat(np.arange(batch_size), num_rois)\n","\n","            # # Print flattened shapes to confirm consistency\n","            # print(\"Flattened rois_flat shape (expected [batch_size * num_rois, 4]):\", rois_flat.shape)\n","            # print(\"Flattened box_indices shape (expected [batch_size * num_rois]):\", box_indices.shape)\n","\n","\n","            tf.config.run_functions_eagerly(True)\n","\n","            # # Print shapes of inputs and targets to ensure they match model expectations\n","            # print(f\"Shape of X_batch: {X_batch.shape}\")\n","            # print(f\"Shape of batch_X2: {batch_X2.shape}\")\n","            # print(f\"Shape of batch_Y1: {batch_Y1.shape}\")\n","            # print(f\"Shape of batch_Y2: {batch_Y2.shape}\")\n","\n","            # Check shapes and attempt the train step\n","            try:\n","                loss_class = model_classifier.train_on_batch([X_batch, batch_X2], [batch_Y1, batch_Y2])\n","            except tf.errors.InvalidArgumentError as e:\n","                print(\"Error during train_on_batch:\", e)\n","                raise e\n","\n","\n","\n","            # print(\"Raw loss_class output:\", loss_class)\n","            # print(\"Type of loss_class output:\", type(loss_class))\n","\n","            # # Check if loss_class is a list, tuple, or numpy array\n","            # if isinstance(loss_class, (list, tuple)):\n","            #     print(\"Loss_class output contains multiple items.\")\n","            #     for i, item in enumerate(loss_class):\n","            #         print(f\"Item {i}: Value = {item}, Type = {type(item)}\")\n","            #         # If item is an array, print its shape\n","            #         if hasattr(item, 'shape'):\n","            #             print(f\"Shape of item {i}:\", item.shape)\n","            # elif isinstance(loss_class, np.ndarray):\n","            #     print(\"Loss_class is a numpy array with shape:\", loss_class.shape)\n","            # else:\n","            #     print(\"Loss_class output is a scalar or unknown type.\")\n","\n","\n","\n","            # Store and print loss\n","            if isinstance(loss_class, (list, tuple, np.ndarray)):\n","                losses[iter_num, 2] = loss_class[0]\n","                losses[iter_num, 3] = loss_class[1] if len(loss_class) > 1 else 0\n","                losses[iter_num, 4] = loss_class[2] if len(loss_class) > 2 else 0\n","            else:\n","                print(\"No loss class\")\n","                losses[iter_num, 2] = loss_class\n","                losses[iter_num, 3] = 0\n","                losses[iter_num, 4] = 0\n","\n","            # print(f\"Classifier Losses - Class: {losses[iter_num, 2]}, Regr: {losses[iter_num, 3]}, Accuracy: {losses[iter_num, 4]}\")\n","# Collect true and predicted boxes\n","            for idx in range(batch_size):\n","                true_boxes_list.extend(img_data_batch[idx]['bboxes'])\n","                predicted_boxes_list.extend(R_batch[idx])\n","            iter_num += 1\n","            progbar.update(iter_num, [\n","                ('rpn_cls', np.mean(losses[:iter_num, 0])),\n","                ('rpn_regr', np.mean(losses[:iter_num, 1])),\n","                ('final_cls', np.mean(losses[:iter_num, 2])),\n","                ('final_regr', np.mean(losses[:iter_num, 3])),\n","                ('class_acc', np.mean(losses[:iter_num, 4]))\n","            ])\n","            epoch_loss += np.sum(losses[iter_num - 1, :])\n","\n","            # Log batch-wise losses\n","            batch_losses_log.append({\n","                'epoch': epoch_num + 1,\n","                'batch': iter_num,\n","                'rpn_cls_loss': losses[iter_num - 1, 0],\n","                'rpn_regr_loss': losses[iter_num - 1, 1],\n","                'class_cls_loss': losses[iter_num - 1, 2],\n","                'class_regr_loss': losses[iter_num - 1, 3],\n","                # 'class_acc': losses[iter_num - 1, 4]\n","            })\n","\n","            # Collect true and predicted boxes\n","            for idx in range(batch_size):\n","                true_boxes_list.extend(img_data_batch[idx]['bboxes'])\n","                predicted_boxes_list.extend(R_batch[idx])\n","\n","            if iter_num >= epoch_length:\n","                break\n","\n","        except StopIteration:\n","            print(\"Reached the end of the dataset, resetting the generator for the next epoch.\")\n","            break\n","\n","        except Exception as e:\n","            print(\"Exception encountered:\")\n","            traceback.print_exc()\n","            raise e\n","\n","    # Epoch summary\n","    loss_rpn_cls = np.mean(losses[:, 0])\n","    loss_rpn_regr = np.mean(losses[:, 1])\n","    loss_class_cls = np.mean(losses[:, 2])\n","    loss_class_regr = np.mean(losses[:, 3])\n","    class_acc = np.mean(losses[:, 4])\n","\n","    curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n","    elapsed_time = round((time.time() - start_time) / 60, 2)\n","\n","    # Debug epoch summary\n","    print(f\"\\n--- End of Epoch {epoch_num + 1} Summary ---\")\n","    print(f\"Mean RPN Class Loss: {loss_rpn_cls:.4f}, Mean RPN Regression Loss: {loss_rpn_regr:.4f}\")\n","    print(f\"Mean Classifier Class Loss: {loss_class_cls:.4f}, Mean Classifier Regression Loss: {loss_class_regr:.4f}\")\n","    print(f\"Mean Classifier Accuracy: {class_acc:.4f}, Current Loss: {curr_loss:.4f}, Best Loss: {best_loss:.4f}\")\n","    print(f\"Elapsed Time: {elapsed_time} mins\\n\")\n","\n","    # print(\"true boxes list\",true_boxes_list)\n","    # print(\"predicted boxes\",predicted_boxes_list)\n","    # print(len(true_boxes_list), len(predicted_boxes_list))\n","    # mAP = compute_map(true_boxes_list, predicted_boxes_list, iou_threshold=0.5)\n","    # print(f\"Mean Average Precision (mAP): {mAP}\")\n","\n","    # Save metrics to record_df\n","    new_row = pd.DataFrame([{\n","        'mean_overlapping_bboxes': np.mean([num_pos for _, _, _, _, num_pos in data_gen_train]),\n","        # 'class_acc': class_acc,\n","        'loss_rpn_cls': loss_rpn_cls,\n","        # 'loss_rpn_regr': loss_rpn_regr,\n","        'loss_class_cls': loss_class_cls,\n","        'loss_class_regr': loss_class_regr,\n","        'curr_loss': curr_loss,\n","        'elapsed_time': elapsed_time,\n","        'mAP': 0\n","    }])\n","    record_df = pd.concat([record_df, new_row], ignore_index=True, sort=False)\n","    # print(\"New row to be added (new_row):\")\n","    # print(new_row)  # Check contents of new_row\n","    # print(\"After dropping NaNs in new_row:\")\n","    # print(new_row.dropna())  # Confirm what remains after dropna()\n","    # print(\"Existing record_df before concatenation:\")\n","    # print(record_df)  # Check if record_df has prior data\n","\n","    # Check if weights have changed to confirm updates\n","    weights_updated = False\n","    for layer_name, initial_weight, updated_weight in zip([layer.name for layer in model_rpn.layers], initial_weights_rpn, model_rpn.get_weights()):\n","        if initial_weight is not None and updated_weight is not None and not np.allclose(initial_weight, updated_weight):\n","            print(f\"Debug: Model weights in layer '{layer_name}' have been updated.\")\n","            weights_updated = True\n","    if weights_updated:\n","        print(\"Debug: Weights updated for this epoch.\")\n","\n","    if curr_loss < best_loss- improvement_threshold:\n","        best_loss = curr_loss\n","        model_all.save_weights(C.model_path)\n","        no_improvement_count = 0\n","        print(\"New best model saved.\")\n","    else:\n","        no_improvement_count += 1\n","        print(f\"No improvement. Incrementing no improvement count to {no_improvement_count}.\")\n","\n","    if no_improvement_count >= patience:\n","        print(f\"Early stopping triggered after {no_improvement_count} epochs without improvement.\")\n","        break\n","\n","record_df.to_csv(f\"{model_dir}/training_record.csv\", index=False)\n","print('Training complete.')"],"metadata":{"id":"WhoTsNWpXgBo","executionInfo":{"status":"ok","timestamp":1731704285415,"user_tz":-120,"elapsed":6096242,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fc51edac-2062-42d1-9a58-62293d2d17ab"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing annotation files\n","idx=2943Epoch length: 250\n","Initial weights check:\n","RPN Model weights are initialized.\n","Classifier Model weights are initialized.\n","\n","Epoch 1/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:40\u001b[0m 3s/step - rpn_cls: 1382.4314 - rpn_regr: 0.0000e+00 - final_cls: 3.2062 - final_regr: 0.8679 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:16\u001b[0m 3s/step - rpn_cls: 1241.3836 - rpn_regr: 0.0000e+00 - final_cls: 3.1979 - final_regr: 0.8734 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 2s/step - rpn_cls: 551.4765 - rpn_regr: 0.0000e+00 - final_cls: 2.7097 - final_regr: 0.8604 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 1 Summary ---\n","Mean RPN Class Loss: 211.0819, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 2.1246, Mean Classifier Regression Loss: 0.8282\n","Mean Classifier Accuracy: 0.0000, Current Loss: 214.0347, Best Loss: inf\n","Elapsed Time: 10.98 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n","<ipython-input-44-8d3b8091497c>:297: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n","  record_df = pd.concat([record_df, new_row], ignore_index=True, sort=False)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 2/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:44\u001b[0m 3s/step - rpn_cls: 39.5476 - rpn_regr: 0.0000e+00 - final_cls: 1.5337 - final_regr: 0.7284 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:20\u001b[0m 3s/step - rpn_cls: 39.1316 - rpn_regr: 0.0000e+00 - final_cls: 1.5380 - final_regr: 0.7252 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 2s/step - rpn_cls: 33.9929 - rpn_regr: 0.0000e+00 - final_cls: 1.6027 - final_regr: 0.6824 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 2 Summary ---\n","Mean RPN Class Loss: 28.6822, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.6553, Mean Classifier Regression Loss: 0.6411\n","Mean Classifier Accuracy: 0.0000, Current Loss: 30.9786, Best Loss: 214.0347\n","Elapsed Time: 21.25 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 3/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:47\u001b[0m 3s/step - rpn_cls: 20.2431 - rpn_regr: 0.0000e+00 - final_cls: 1.6534 - final_regr: 0.6255 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:22\u001b[0m 3s/step - rpn_cls: 20.1298 - rpn_regr: 0.0000e+00 - final_cls: 1.6547 - final_regr: 0.6271 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m614s\u001b[0m 2s/step - rpn_cls: 18.5705 - rpn_regr: 0.0000e+00 - final_cls: 1.6869 - final_regr: 0.6488 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 3 Summary ---\n","Mean RPN Class Loss: 16.8038, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.7162, Mean Classifier Regression Loss: 0.6738\n","Mean Classifier Accuracy: 0.0000, Current Loss: 19.1938, Best Loss: 30.9786\n","Elapsed Time: 31.54 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 4/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:41\u001b[0m 3s/step - rpn_cls: 13.6129 - rpn_regr: 0.0000e+00 - final_cls: 1.7552 - final_regr: 0.7185 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:17\u001b[0m 3s/step - rpn_cls: 13.5611 - rpn_regr: 0.0000e+00 - final_cls: 1.7550 - final_regr: 0.7191 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m605s\u001b[0m 2s/step - rpn_cls: 12.8148 - rpn_regr: 0.0000e+00 - final_cls: 1.7535 - final_regr: 0.7292 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 4 Summary ---\n","Mean RPN Class Loss: 11.9360, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.7467, Mean Classifier Regression Loss: 0.7415\n","Mean Classifier Accuracy: 0.0000, Current Loss: 14.4242, Best Loss: 19.1938\n","Elapsed Time: 41.69 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 5/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:39\u001b[0m 3s/step - rpn_cls: 10.2590 - rpn_regr: 0.0000e+00 - final_cls: 1.7170 - final_regr: 0.7651 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:15\u001b[0m 3s/step - rpn_cls: 10.2294 - rpn_regr: 0.0000e+00 - final_cls: 1.7155 - final_regr: 0.7654 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m604s\u001b[0m 2s/step - rpn_cls: 9.7970 - rpn_regr: 0.0000e+00 - final_cls: 1.6839 - final_regr: 0.7719 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 5 Summary ---\n","Mean RPN Class Loss: 9.2805, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.6322, Mean Classifier Regression Loss: 0.7814\n","Mean Classifier Accuracy: 0.0000, Current Loss: 11.6941, Best Loss: 14.4242\n","Elapsed Time: 51.83 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 6/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:35\u001b[0m 3s/step - rpn_cls: 8.2583 - rpn_regr: 0.0000e+00 - final_cls: 1.4990 - final_regr: 0.8037 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:10\u001b[0m 3s/step - rpn_cls: 8.2391 - rpn_regr: 0.0000e+00 - final_cls: 1.4961 - final_regr: 0.8041 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m601s\u001b[0m 2s/step - rpn_cls: 7.9522 - rpn_regr: 0.0000e+00 - final_cls: 1.4523 - final_regr: 0.8108 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 6 Summary ---\n","Mean RPN Class Loss: 7.6021, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.3981, Mean Classifier Regression Loss: 0.8190\n","Mean Classifier Accuracy: 0.0000, Current Loss: 9.8192, Best Loss: 11.6941\n","Elapsed Time: 61.9 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 7/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:35\u001b[0m 3s/step - rpn_cls: 6.8996 - rpn_regr: 0.0000e+00 - final_cls: 1.2864 - final_regr: 0.8353 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:09\u001b[0m 3s/step - rpn_cls: 6.8862 - rpn_regr: 0.0000e+00 - final_cls: 1.2843 - final_regr: 0.8356 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m594s\u001b[0m 2s/step - rpn_cls: 6.6836 - rpn_regr: 0.0000e+00 - final_cls: 1.2551 - final_regr: 0.8398 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 7 Summary ---\n","Mean RPN Class Loss: 6.4340, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.2191, Mean Classifier Regression Loss: 0.8450\n","Mean Classifier Accuracy: 0.0000, Current Loss: 8.4981, Best Loss: 9.8192\n","Elapsed Time: 71.86 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 8/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:25\u001b[0m 2s/step - rpn_cls: 5.9264 - rpn_regr: 0.0000e+00 - final_cls: 1.1437 - final_regr: 0.8550 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:01\u001b[0m 3s/step - rpn_cls: 5.9165 - rpn_regr: 0.0000e+00 - final_cls: 1.1416 - final_regr: 0.8551 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m586s\u001b[0m 2s/step - rpn_cls: 5.7658 - rpn_regr: 0.0000e+00 - final_cls: 1.1138 - final_regr: 0.8578 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 8 Summary ---\n","Mean RPN Class Loss: 5.5789, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.0847, Mean Classifier Regression Loss: 0.8608\n","Mean Classifier Accuracy: 0.0000, Current Loss: 7.5245, Best Loss: 8.4981\n","Elapsed Time: 81.7 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 9/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:24\u001b[0m 2s/step - rpn_cls: 5.1949 - rpn_regr: 0.0000e+00 - final_cls: 1.0451 - final_regr: 0.8656 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:01\u001b[0m 3s/step - rpn_cls: 5.1873 - rpn_regr: 0.0000e+00 - final_cls: 1.0448 - final_regr: 0.8657 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 2s/step - rpn_cls: 5.0709 - rpn_regr: 0.0000e+00 - final_cls: 1.0402 - final_regr: 0.8664 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 9 Summary ---\n","Mean RPN Class Loss: 4.9257, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.0317, Mean Classifier Regression Loss: 0.8674\n","Mean Classifier Accuracy: 0.0000, Current Loss: 6.8248, Best Loss: 7.5245\n","Elapsed Time: 91.53 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","\n","Epoch 10/10\n","\u001b[1m 46/250\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:25\u001b[0m 2s/step - rpn_cls: 4.6251 - rpn_regr: 0.0000e+00 - final_cls: 1.0150 - final_regr: 0.8693 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000336587.jpg\n","\u001b[1m 58/250\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8:01\u001b[0m 3s/step - rpn_cls: 4.6191 - rpn_regr: 0.0000e+00 - final_cls: 1.0150 - final_regr: 0.8693 - class_acc: 0.0000e+00Warning: No positive anchors generated for image /content/drive/MyDrive/datasets/coco/train/000000014736.jpg\n","\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m585s\u001b[0m 2s/step - rpn_cls: 4.5264 - rpn_regr: 0.0000e+00 - final_cls: 1.0121 - final_regr: 0.8695 - class_acc: 0.0000e+00\n","\n","--- End of Epoch 10 Summary ---\n","Mean RPN Class Loss: 4.4105, Mean RPN Regression Loss: 0.0000\n","Mean Classifier Class Loss: 1.0066, Mean Classifier Regression Loss: 0.8701\n","Mean Classifier Accuracy: 0.0000, Current Loss: 6.2872, Best Loss: 6.8248\n","Elapsed Time: 101.35 mins\n","\n","Debug: Model weights in layer 'input_layer' have been updated.\n","Debug: Model weights in layer 'block1_conv1' have been updated.\n","Debug: Model weights in layer 'block1_conv2' have been updated.\n","Debug: Model weights in layer 'block1_pool' have been updated.\n","Debug: Model weights in layer 'block2_conv1' have been updated.\n","Debug: Model weights in layer 'block2_conv2' have been updated.\n","Debug: Model weights in layer 'block2_pool' have been updated.\n","Debug: Model weights in layer 'block3_conv1' have been updated.\n","Debug: Model weights in layer 'block3_conv2' have been updated.\n","Debug: Model weights in layer 'block3_conv3' have been updated.\n","Debug: Model weights in layer 'block3_pool' have been updated.\n","Debug: Model weights in layer 'block4_conv1' have been updated.\n","Debug: Model weights in layer 'block4_conv2' have been updated.\n","Debug: Model weights in layer 'block4_conv3' have been updated.\n","Debug: Model weights in layer 'block4_pool' have been updated.\n","Debug: Model weights in layer 'block5_conv1' have been updated.\n","Debug: Model weights in layer 'block5_conv2' have been updated.\n","Debug: Model weights in layer 'block5_conv3' have been updated.\n","Debug: Model weights in layer 'rpn_conv1' have been updated.\n","Debug: Model weights in layer 'rpn_out_class' have been updated.\n","Debug: Model weights in layer 'rpn_out_regress' have been updated.\n","Debug: Weights updated for this epoch.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n","  return _methods._mean(a, axis=axis, dtype=dtype,\n","/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n","  ret = ret.dtype.type(ret / rcount)\n"]},{"output_type":"stream","name":"stdout","text":["New best model saved.\n","Training complete.\n"]}]},{"cell_type":"code","source":["import traceback\n","import numpy as np\n","import os\n","from collections import Counter\n","from sklearn.metrics import confusion_matrix, classification_report\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from keras.utils import Progbar\n","\n","# Load test data\n","test_imgs, _, class_mapping = get_data(test_path, limit=12)  # Adjust limit as needed\n","\n","# Ensure model path is correctly formatted\n","C.model_path = C.model_path.replace(\".hdf5\", \".weights.h5\")\n","if not os.path.exists(C.model_path):\n","    raise FileNotFoundError(f\"Model weights file not found: {C.model_path}\")\n","\n","# Load model weights\n","try:\n","    model_rpn.load_weights(C.model_path)\n","    model_classifier.load_weights(C.model_path)\n","    print(\"Model weights loaded successfully.\")\n","except Exception as e:\n","    print(\"Error loading model weights:\")\n","    traceback.print_exc()\n","    raise e\n","\n","# Evaluation settings\n","iou_threshold = 0.4  # Threshold for IoU calculations\n","batch_size = 4\n","test_batches = len(test_imgs) // batch_size\n","\n","# Progress bar for testing\n","progbar = Progbar(test_batches)\n","\n","# Initialize arrays for true and predicted labels\n","y_true = []\n","y_pred = []\n","true_boxes_list = []  # Store true boxes\n","predicted_boxes_list = []  # Store predicted boxes\n","\n","# Testing loop\n","for batch_idx, (X_batch, _, img_data_batch, _, _, _) in enumerate(\n","    get_anchor_gt(test_imgs, C, get_img_output_length, mode=\"test\", batch_size=batch_size)\n","):\n","    try:\n","        # Preprocess batch\n","        X_batch = np.transpose(X_batch, (0, 2, 3, 1))  # Ensure correct input shape\n","        if X_batch.size == 0:\n","            print(f\"Skipping empty batch {batch_idx + 1}.\")\n","            continue\n","\n","        # Predict RPN outputs\n","        P_rpn = model_rpn.predict_on_batch(X_batch)\n","\n","        # Generate ROIs for each image in the batch\n","        R_batch = [\n","            rpn_to_roi(\n","                np.expand_dims(P_rpn[0][img_idx], axis=0),\n","                np.expand_dims(P_rpn[1][img_idx], axis=0),\n","                C,\n","                use_regr=True,\n","                overlap_thresh=0.5,\n","                max_boxes=300,\n","                dim_ordering=\"channels_last\",\n","            )\n","            for img_idx in range(batch_size)\n","        ]\n","\n","        # Prepare ROIs for classifier\n","        batch_X2, batch_Y1, batch_Y2, IoUs_batch = calc_iou(R_batch, img_data_batch, C, class_mapping)\n","\n","        # Skip if no valid ROIs\n","        if batch_X2 is None:\n","            print(f\"No valid ROIs for batch {batch_idx + 1}. Skipping.\")\n","            continue\n","\n","        # Predict using classifier model\n","        class_probs, _ = model_classifier.predict_on_batch([X_batch, batch_X2])\n","\n","        # Match predictions and ground truths\n","        for img_idx in range(batch_size):\n","            # Get true labels\n","            true_labels = batch_Y1[img_idx].argmax(axis=-1)  # One-hot encoded to index\n","            y_true.extend(true_labels)\n","\n","            # Get predicted labels\n","            predicted_labels = class_probs[img_idx].argmax(axis=-1)  # Probabilities to index\n","            y_pred.extend(predicted_labels)\n","\n","            # Populate true boxes\n","            true_boxes_list.append(img_data_batch[img_idx][\"bboxes\"])  # Append ground truth bboxes\n","\n","            # Populate predicted boxes\n","            predicted_boxes_list.append(R_batch[img_idx])  # Append predicted ROIs\n","\n","        progbar.update(batch_idx + 1)\n","\n","    except Exception as e:\n","        print(f\"Exception encountered in batch {batch_idx + 1}:\")\n","        traceback.print_exc()\n","        continue\n","\n","# Check if y_true and y_pred are populated\n","if not y_true or not y_pred:\n","    raise ValueError(\"y_true or y_pred is empty. Ensure that the testing loop is populating them correctly.\")\n","print(\"y_true:\", Counter(y_true))  # Check distribution of true labels\n","print(\"y_pred:\", Counter(y_pred))  # Check distribution of predicted labels\n","\n","# Assuming these are the class mappings and labels based on your data\n","# class_mapping = {0: \"aeroplane\", 9: \"bg\", 5: \"car\", 6: \"cat\", 7: \"bus\", 24: \"unknown\"}\n","class_mapping = {0: \"aeroplane\", 9: \"bg\", 5: \"car\", 6: \"cat\", 7: \"bus\", 24: \"bg\"}\n","\n","class_names = list(class_mapping.values())\n","class_indices = list(class_mapping.keys())\n","\n","# Provided y_true and y_pred values (expanded from counts)\n","y_true = [9]*239 + [0]*113 + [5]*32 + [6]*8 + [7]*8  # Full list based on Counter\n","y_pred = [24]*305 + [0]*95  # Full list based on Counter\n","\n","# Calculate and print the number of labels per class in y_true\n","label_counts = Counter(y_true)\n","print(\"\\nNumber of labels per class in the test dataset:\")\n","for label_idx, count in label_counts.items():\n","    label_name = class_names[class_indices.index(label_idx)]\n","    print(f\"{label_name}: {count}\")\n","\n","# Generate confusion matrix using y_true and y_pred with provided class indices\n","conf_matrix = confusion_matrix(y_true, y_pred, labels=class_indices)\n","\n","# Display confusion matrix as a heatmap\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(conf_matrix, annot=True, fmt=\"d\", xticklabels=class_names, yticklabels=class_names, cmap=\"Blues\")\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()\n","\n","# Generate and print the classification report\n","print(\"\\nClassification Report:\")\n","print(\n","    classification_report(\n","        y_true, y_pred, labels=class_indices, target_names=class_names, zero_division=0\n","    )\n",")\n","\n","# Compute mAP\n","try:\n","    mAP = compute_map(true_boxes_list, predicted_boxes_list, iou_threshold=iou_threshold)\n","    print(f\"\\nMean Average Precision (mAP) on test data: {mAP:.4f}\")\n","except Exception as e:\n","    print(\"Error computing mAP:\")\n","    traceback.print_exc()\n","\n","print(\"Testing complete.\")"],"metadata":{"id":"ayP0kQuCRNSC","executionInfo":{"status":"ok","timestamp":1731706232364,"user_tz":-120,"elapsed":28114,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"051dee04-f2b0-4efd-ce84-b4d949b7f0e8"},"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Parsing annotation files\n","idx=14Model weights loaded successfully.\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n","\u001b[1m4/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n","\u001b[1m5/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step\n","\u001b[1m6/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step\n","\u001b[1m7/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step\n","\u001b[1m8/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step\n","\u001b[1m9/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step\n","\u001b[1m10/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step\n","\u001b[1m11/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step\n","\u001b[1m12/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 1s/step\n","\u001b[1m13/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 1s/step\n","\u001b[1m14/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 1s/step\n","\u001b[1m15/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step\n","\u001b[1m16/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step\n","\u001b[1m17/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step\n","\u001b[1m18/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step\n","\u001b[1m19/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step\n","\u001b[1m20/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step\n","\u001b[1m21/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 1s/step\n","\u001b[1m22/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 1s/step\n","\u001b[1m23/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 1s/step\n","\u001b[1m24/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 1s/step\n","\u001b[1m25/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1s/step\n","y_true: Counter({9: 239, 0: 113, 5: 32, 6: 8, 7: 8})\n","y_pred: Counter({24: 305, 0: 95})\n","\n","Number of labels per class in the test dataset:\n","bg: 239\n","aeroplane: 113\n","car: 32\n","cat: 8\n","bus: 8\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x800 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","   aeroplane       0.49      0.42      0.45       113\n","          bg       0.00      0.00      0.00       239\n","         car       0.00      0.00      0.00        32\n","         cat       0.00      0.00      0.00         8\n","         bus       0.00      0.00      0.00         8\n","          bg       0.00      0.00      0.00         0\n","\n","    accuracy                           0.12       400\n","   macro avg       0.08      0.07      0.08       400\n","weighted avg       0.14      0.12      0.13       400\n","\n","Error computing mAP:\n","Testing complete.\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"<ipython-input-54-adc096473305>\", line 149, in <cell line: 148>\n","    mAP = compute_map(true_boxes_list, predicted_boxes_list, iou_threshold=iou_threshold)\n","  File \"<ipython-input-43-c32a0b759c26>\", line 26, in compute_map\n","    max_iou = max(max_iou, current_iou)\n","NameError: name 'current_iou' is not defined\n"]}]},{"cell_type":"code","source":["\n","# Δημιουργία του DataFrame από τα δεδομένα του Classification Report\n","data = {\n","    \"Class\": [\"aeroplane\", \"bg\", \"car\", \"cat\", \"bus\", \"accuracy\", \"macro avg\", \"weighted avg\"],\n","    \"Precision\": [0.49, 0.00, 0.00, 0.00, 0.00, None, 0.08, 0.14],\n","    \"Recall\": [0.42, 0.00, 0.00, 0.00, 0.00, 0.12, 0.07, 0.12],\n","    \"F1-score\": [0.45, 0.00, 0.00, 0.00, 0.00, None, 0.08, 0.13],\n","    \"Support\": [113, 239, 32, 8, 8, 400, 400, 400]\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Plotting το DataFrame σε ένα heatmap\n","plt.figure(figsize=(10, 4))\n","sns.heatmap(df.set_index(\"Class\"), annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=False, linewidths=.5)\n","plt.title(\"Classification Report\")\n","plt.savefig(\"classification_report.png\")\n","plt.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"id":"aMOtba2-u2e6","executionInfo":{"status":"ok","timestamp":1731706856554,"user_tz":-120,"elapsed":1484,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"outputId":"2de53e7c-a3e8-4fbd-bf46-b7928de7d10e"},"execution_count":55,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import seaborn as sns\n","\n","# Set up Seaborn style for modern, visually appealing plots\n","sns.set(style=\"whitegrid\", palette=\"colorblind\")\n","\n","# Set up the number of epochs\n","epochs = len(record_df)\n","\n","# Define a color palette\n","palette = sns.color_palette(\"husl\", 8)  # Husl is a perceptually uniform color palette\n","\n","# Plot mean_overlapping_bboxes and class_acc, if available\n","if 'mean_overlapping_bboxes' in record_df.columns and 'class_acc' in record_df.columns:\n","    plt.figure(figsize=(15, 5))\n","\n","    # Mean Overlapping Bboxes\n","    plt.subplot(1, 2, 1)\n","    plt.plot(np.arange(0, epochs), record_df['mean_overlapping_bboxes'], color=palette[0], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['mean_overlapping_bboxes'], color=palette[0], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Mean Overlapping Bboxes')\n","    plt.title('Mean Overlapping Bboxes')\n","\n","    # Class Accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(np.arange(0, epochs), record_df['class_acc'], color=palette[1], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['class_acc'], color=palette[1], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Class Accuracy')\n","    plt.title('Class Accuracy')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Skipping plot for 'mean_overlapping_bboxes' or 'class_acc' as they are not in record_df.\")\n","\n","# Plot loss_rpn_cls and loss_rpn_regr, if available\n","if 'loss_rpn_cls' in record_df.columns and 'loss_rpn_regr' in record_df.columns:\n","    plt.figure(figsize=(15, 5))\n","\n","    # RPN Classification Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(np.arange(0, epochs), record_df['loss_rpn_cls'], color=palette[2], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_rpn_cls'], color=palette[2], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('RPN Classification Loss')\n","    plt.title('RPN Classification Loss')\n","\n","    # RPN Regression Loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(np.arange(0, epochs), record_df['loss_rpn_regr'], color=palette[3], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_rpn_regr'], color=palette[3], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('RPN Regression Loss')\n","    plt.title('RPN Regression Loss')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Skipping plot for 'loss_rpn_cls' or 'loss_rpn_regr' as they are not in record_df.\")\n","\n","# Plot loss_class_cls and loss_class_regr, if available\n","if 'loss_class_cls' in record_df.columns and 'loss_class_regr' in record_df.columns:\n","    plt.figure(figsize=(15, 5))\n","\n","    # Classifier Classification Loss\n","    plt.subplot(1, 2, 1)\n","    plt.plot(np.arange(0, epochs), record_df['loss_class_cls'], color=palette[4], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_class_cls'], color=palette[4], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Classifier Classification Loss')\n","    plt.title('Classifier Classification Loss')\n","\n","    # Classifier Regression Loss\n","    plt.subplot(1, 2, 2)\n","    plt.plot(np.arange(0, epochs), record_df['loss_class_regr'], color=palette[5], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_class_regr'], color=palette[5], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Classifier Regression Loss')\n","    plt.title('Classifier Regression Loss')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Skipping plot for 'loss_class_cls' or 'loss_class_regr' as they are not in record_df.\")\n","\n","# Plot total loss (curr_loss), if available\n","if 'curr_loss' in record_df.columns:\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(np.arange(0, epochs), record_df['curr_loss'], color=palette[6], linewidth=2, marker='o')\n","    plt.fill_between(np.arange(0, epochs), record_df['curr_loss'], color=palette[6], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Total Loss')\n","    plt.title('Total Loss Across Epochs')\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Skipping plot for 'curr_loss' as it is not in record_df.\")\n","\n","# Combined view of all loss components for easy comparison, if available\n","if {'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr'}.issubset(record_df.columns):\n","    plt.figure(figsize=(15, 5))\n","    plt.plot(np.arange(0, epochs), record_df['loss_rpn_cls'], color=palette[2], label='RPN Classification Loss', linewidth=2)\n","    plt.plot(np.arange(0, epochs), record_df['loss_rpn_regr'], color=palette[3], label='RPN Regression Loss', linewidth=2)\n","    plt.plot(np.arange(0, epochs), record_df['loss_class_cls'], color=palette[4], label='Classifier Classification Loss', linewidth=2)\n","    plt.plot(np.arange(0, epochs), record_df['loss_class_regr'], color=palette[5], label='Classifier Regression Loss', linewidth=2)\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_rpn_cls'], color=palette[2], alpha=0.1)\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_rpn_regr'], color=palette[3], alpha=0.1)\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_class_cls'], color=palette[4], alpha=0.1)\n","    plt.fill_between(np.arange(0, epochs), record_df['loss_class_regr'], color=palette[5], alpha=0.1)\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss Value')\n","    plt.title('Loss Components Comparison')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","else:\n","    print(\"Skipping combined loss components plot as some columns are missing in record_df.\")"],"metadata":{"id":"HJHTEyD5g3vk","executionInfo":{"status":"error","timestamp":1731704333631,"user_tz":-120,"elapsed":13,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}},"colab":{"base_uri":"https://localhost:8080/","height":851},"outputId":"191c0db3-4f73-4fc2-caa1-873a1052400f"},"execution_count":46,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-46-d615ebded15c>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_overlapping_bboxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_between\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_overlapping_bboxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Mean Overlapping Bboxes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mfill_between\u001b[0;34m(x, y1, y2, where, interpolate, step, data, **kwargs)\u001b[0m\n\u001b[1;32m   3097\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3098\u001b[0m ) -> PolyCollection:\n\u001b[0;32m-> 3099\u001b[0;31m     return gca().fill_between(\n\u001b[0m\u001b[1;32m   3100\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3101\u001b[0m         \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1463\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mfill_between\u001b[0;34m(self, x, y1, y2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5502\u001b[0m     def fill_between(self, x, y1, y2=0, where=None, interpolate=False,\n\u001b[1;32m   5503\u001b[0m                      step=None, **kwargs):\n\u001b[0;32m-> 5504\u001b[0;31m         return self._fill_between_x_or_y(\n\u001b[0m\u001b[1;32m   5505\u001b[0m             \u001b[0;34m\"x\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5506\u001b[0m             where=where, interpolate=interpolate, step=step, **kwargs)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36m_fill_between_x_or_y\u001b[0;34m(self, ind_dir, ind, dep1, dep2, where, interpolate, step, **kwargs)\u001b[0m\n\u001b[1;32m   5402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5403\u001b[0m         \u001b[0;31m# Handle united data, such as dates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5404\u001b[0;31m         ind, dep1, dep2 = map(\n\u001b[0m\u001b[1;32m   5405\u001b[0m             ma.masked_invalid, self._process_unit_info(\n\u001b[1;32m   5406\u001b[0m                 [(ind_dir, ind), (dep_dir, dep1), (dep_dir, dep2)], kwargs))\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/ma/core.py\u001b[0m in \u001b[0;36mmasked_invalid\u001b[0;34m(a, copy)\u001b[0m\n\u001b[1;32m   2358\u001b[0m     \"\"\"\n\u001b[1;32m   2359\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasked_where\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2361\u001b[0m     \u001b[0;31m# masked_invalid previously never returned nomask as a mask and doing so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[0;31m# threw off matplotlib (gh-22842).  So use shrink=False:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1500x500 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kt-1Grs90oD3","executionInfo":{"status":"aborted","timestamp":1731704333661,"user_tz":-120,"elapsed":41,"user":{"displayName":"Νίκος Μπρέμπος","userId":"02938906472153568206"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Set up the number of epochs\n","epochs = len(record_df)\n","\n","# Plot mean_overlapping_bboxes and class_acc\n","plt.figure(figsize=(15, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(np.arange(0, epochs), record_df['mean_overlapping_bboxes'], 'r')\n","plt.xlabel('Epoch')\n","plt.ylabel('Mean Overlapping Bboxes')\n","plt.title('Mean Overlapping Bboxes')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(np.arange(0, epochs), record_df['class_acc'], 'r')\n","plt.xlabel('Epoch')\n","plt.ylabel('Class Accuracy')\n","plt.title('Class Accuracy')\n","plt.show()\n","\n","# Plot loss_rpn_cls and loss_rpn_regr\n","plt.figure(figsize=(15, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(np.arange(0, epochs), record_df['loss_rpn_cls'], 'b')\n","plt.xlabel('Epoch')\n","plt.ylabel('RPN Classification Loss')\n","plt.title('RPN Classification Loss')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(np.arange(0, epochs), record_df['loss_rpn_regr'], 'g')\n","plt.xlabel('Epoch')\n","plt.ylabel('RPN Regression Loss')\n","plt.title('RPN Regression Loss')\n","plt.show()\n","\n","# Plot loss_class_cls and loss_class_regr\n","plt.figure(figsize=(15, 5))\n","plt.subplot(1, 2, 1)\n","plt.plot(np.arange(0, epochs), record_df['loss_class_cls'], 'c')\n","plt.xlabel('Epoch')\n","plt.ylabel('Classifier Classification Loss')\n","plt.title('Classifier Classification Loss')\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(np.arange(0, epochs), record_df['loss_class_regr'], 'm')\n","plt.xlabel('Epoch')\n","plt.ylabel('Classifier Regression Loss')\n","plt.title('Classifier Regression Loss')\n","plt.show()\n","\n","# Plot total loss (curr_loss)\n","plt.figure(figsize=(10, 5))\n","plt.plot(np.arange(0, epochs), record_df['curr_loss'], 'r')\n","plt.xlabel('Epoch')\n","plt.ylabel('Total Loss')\n","plt.title('Total Loss Across Epochs')\n","plt.show()\n","\n","# Optional: Combined view of all loss components for easy comparison\n","plt.figure(figsize=(15, 5))\n","plt.plot(np.arange(0, epochs), record_df['loss_rpn_cls'], 'b', label='RPN Classification Loss')\n","plt.plot(np.arange(0, epochs), record_df['loss_rpn_regr'], 'g', label='RPN Regression Loss')\n","plt.plot(np.arange(0, epochs), record_df['loss_class_cls'], 'c', label='Classifier Classification Loss')\n","plt.plot(np.arange(0, epochs), record_df['loss_class_regr'], 'm', label='Classifier Regression Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss Value')\n","plt.title('Loss Components Comparison')\n","plt.legend()\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[{"file_id":"1rHZOGsDA2n9iGwKk2YpseKnX2hxPax6W","timestamp":1731167912040},{"file_id":"1D96ezaaoJhfUnS7SZH0TSsxBw-ZhYsIL","timestamp":1730713206627},{"file_id":"1-PAFveUnvA8d4ESIlwX9t3RA1HqrOe1A","timestamp":1730673828908},{"file_id":"1ANG0_CXPdAaA2zp2uBACO8uaMMj_HYbw","timestamp":1730557349186},{"file_id":"17SE0orLLyqCMG_w-0cCXfTtDIXuZ9i7x","timestamp":1729633990541}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}